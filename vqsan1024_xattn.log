Loading config configs/Base-coco-stuff-10K-171.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.
Command Line Args: Namespace(config_file='configs/san_clip_vit_res4_coco.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50155', opts=['SOLVER.IMS_PER_BATCH', '2', 'OUTPUT_DIR', 'output/vqsan1024_xattn', 'WANDB.NAME', 'san_wandb_log', 'DATASETS.TEST', "('coco_2017_test_stuff_sem_seg',)"])
[12/02 00:32:17 detectron2]: Rank of current process: 0. World size: 1
[12/02 00:32:18 detectron2]: Environment info:
-------------------------------  -------------------------------------------------------------------------------
sys.platform                     linux
Python                           3.9.18 (main, Sep 11 2023, 13:41:44) [GCC 11.2.0]
numpy                            1.22.4
detectron2                       0.6 @/home/zjy/.conda/envs/san/lib/python3.9/site-packages/detectron2
Compiler                         GCC 9.3
CUDA compiler                    CUDA 11.3
detectron2 arch flags            8.6
DETECTRON2_ENV_MODULE            <not set>
PyTorch                          1.12.1+cu113 @/home/zjy/.conda/envs/san/lib/python3.9/site-packages/torch
PyTorch debug build              False
torch._C._GLIBCXX_USE_CXX11_ABI  False
GPU available                    Yes
GPU 0                            NVIDIA GeForce RTX 3080 (arch=8.6)
Driver version                   515.48.07
CUDA_HOME                        /usr/local/cuda
Pillow                           9.3.0
torchvision                      0.13.1+cu113 @/home/zjy/.conda/envs/san/lib/python3.9/site-packages/torchvision
torchvision arch flags           3.5, 5.0, 6.0, 7.0, 7.5, 8.0, 8.6
fvcore                           0.1.5.post20221221
iopath                           0.1.9
cv2                              4.8.1
-------------------------------  -------------------------------------------------------------------------------
PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - CUDA Runtime 11.3
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86
  - CuDNN 8.3.2  (built against CUDA 11.5)
  - Magma 2.5.2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

[12/02 00:32:18 detectron2]: Command line arguments: Namespace(config_file='configs/san_clip_vit_res4_coco.yaml', resume=False, eval_only=False, num_gpus=1, num_machines=1, machine_rank=0, dist_url='tcp://127.0.0.1:50155', opts=['SOLVER.IMS_PER_BATCH', '2', 'OUTPUT_DIR', 'output/vqsan1024_xattn', 'WANDB.NAME', 'san_wandb_log', 'DATASETS.TEST', "('coco_2017_test_stuff_sem_seg',)"])
[12/02 00:32:18 detectron2]: Contents of args.config_file=configs/san_clip_vit_res4_coco.yaml:
_BASE_: Base-coco-stuff-10K-171.yaml
MODEL:
  META_ARCHITECTURE: "SAN"
SOLVER:
  BACKBONE_MULTIPLIER: 1.0
[12/02 00:32:18 detectron2]: Running with full config:
CUDNN_BENCHMARK: false
DATALOADER:
  ASPECT_RATIO_GROUPING: true
  FILTER_EMPTY_ANNOTATIONS: true
  NUM_WORKERS: 4
  REPEAT_THRESHOLD: 0.0
  SAMPLER_TRAIN: TrainingSampler
DATASETS:
  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000
  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000
  PROPOSAL_FILES_TEST: []
  PROPOSAL_FILES_TRAIN: []
  TEST:
  - coco_2017_test_stuff_sem_seg
  TRAIN:
  - coco_2017_train_stuff_10k_sem_seg
GLOBAL:
  HACK: 1.0
INPUT:
  COLOR_AUG_SSD: true
  CROP:
    ENABLED: true
    SINGLE_CATEGORY_MAX_AREA: 1.0
    SIZE:
    - 640
    - 640
    TYPE: absolute
  DATASET_MAPPER_NAME: mask_former_semantic
  FORMAT: RGB
  MASK_FORMAT: polygon
  MAX_SIZE_TEST: 2560
  MAX_SIZE_TRAIN: 2560
  MIN_SIZE_TEST: 640
  MIN_SIZE_TRAIN:
  - 320
  - 384
  - 448
  - 512
  - 576
  - 640
  - 704
  - 768
  - 832
  - 896
  - 960
  MIN_SIZE_TRAIN_SAMPLING: choice
  RANDOM_FLIP: horizontal
  SIZE_DIVISIBILITY: 640
MODEL:
  ANCHOR_GENERATOR:
    ANGLES:
    - - -90
      - 0
      - 90
    ASPECT_RATIOS:
    - - 0.5
      - 1.0
      - 2.0
    NAME: DefaultAnchorGenerator
    OFFSET: 0.0
    SIZES:
    - - 32
      - 64
      - 128
      - 256
      - 512
  BACKBONE:
    FREEZE_AT: 2
    NAME: build_resnet_backbone
  DEVICE: cuda
  FLASH: false
  FPN:
    FUSE_TYPE: sum
    IN_FEATURES: []
    NORM: ''
    OUT_CHANNELS: 256
  KEYPOINT_ON: false
  LOAD_PROPOSALS: false
  MASK_ON: false
  META_ARCHITECTURE: SAN
  PANOPTIC_FPN:
    COMBINE:
      ENABLED: true
      INSTANCES_CONFIDENCE_THRESH: 0.5
      OVERLAP_THRESH: 0.5
      STUFF_AREA_LIMIT: 4096
    INSTANCE_LOSS_WEIGHT: 1.0
  PIXEL_MEAN:
  - 103.53
  - 116.28
  - 123.675
  PIXEL_STD:
  - 1.0
  - 1.0
  - 1.0
  PROPOSAL_GENERATOR:
    MIN_SIZE: 0
    NAME: RPN
  RESNETS:
    DEFORM_MODULATED: false
    DEFORM_NUM_GROUPS: 1
    DEFORM_ON_PER_STAGE:
    - false
    - false
    - false
    - false
    DEPTH: 50
    NORM: FrozenBN
    NUM_GROUPS: 1
    OUT_FEATURES:
    - res4
    RES2_OUT_CHANNELS: 256
    RES4_DILATION: 1
    RES5_DILATION: 1
    RES5_MULTI_GRID:
    - 1
    - 2
    - 4
    STEM_OUT_CHANNELS: 64
    STEM_TYPE: deeplab
    STRIDE_IN_1X1: true
    WIDTH_PER_GROUP: 64
  RETINANET:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_WEIGHTS: &id002
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    FOCAL_LOSS_ALPHA: 0.25
    FOCAL_LOSS_GAMMA: 2.0
    IN_FEATURES:
    - p3
    - p4
    - p5
    - p6
    - p7
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.4
    - 0.5
    NMS_THRESH_TEST: 0.5
    NORM: ''
    NUM_CLASSES: 80
    NUM_CONVS: 4
    PRIOR_PROB: 0.01
    SCORE_THRESH_TEST: 0.05
    SMOOTH_L1_LOSS_BETA: 0.1
    TOPK_CANDIDATES_TEST: 1000
  ROI_BOX_CASCADE_HEAD:
    BBOX_REG_WEIGHTS:
    - &id001
      - 10.0
      - 10.0
      - 5.0
      - 5.0
    - - 20.0
      - 20.0
      - 10.0
      - 10.0
    - - 30.0
      - 30.0
      - 15.0
      - 15.0
    IOUS:
    - 0.5
    - 0.6
    - 0.7
  ROI_BOX_HEAD:
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id001
    CLS_AGNOSTIC_BBOX_REG: false
    CONV_DIM: 256
    FC_DIM: 1024
    FED_LOSS_FREQ_WEIGHT_POWER: 0.5
    FED_LOSS_NUM_CLASSES: 50
    NAME: ''
    NORM: ''
    NUM_CONV: 0
    NUM_FC: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
    SMOOTH_L1_BETA: 0.0
    TRAIN_ON_PRED_BOXES: false
    USE_FED_LOSS: false
    USE_SIGMOID_CE: false
  ROI_HEADS:
    BATCH_SIZE_PER_IMAGE: 512
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - 1
    IOU_THRESHOLDS:
    - 0.5
    NAME: Res5ROIHeads
    NMS_THRESH_TEST: 0.5
    NUM_CLASSES: 80
    POSITIVE_FRACTION: 0.25
    PROPOSAL_APPEND_GT: true
    SCORE_THRESH_TEST: 0.05
  ROI_KEYPOINT_HEAD:
    CONV_DIMS:
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    - 512
    LOSS_WEIGHT: 1.0
    MIN_KEYPOINTS_PER_IMAGE: 1
    NAME: KRCNNConvDeconvUpsampleHead
    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true
    NUM_KEYPOINTS: 17
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  ROI_MASK_HEAD:
    CLS_AGNOSTIC_MASK: false
    CONV_DIM: 256
    NAME: MaskRCNNConvUpsampleHead
    NORM: ''
    NUM_CONV: 0
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 0
    POOLER_TYPE: ROIAlignV2
  RPN:
    BATCH_SIZE_PER_IMAGE: 256
    BBOX_REG_LOSS_TYPE: smooth_l1
    BBOX_REG_LOSS_WEIGHT: 1.0
    BBOX_REG_WEIGHTS: *id002
    BOUNDARY_THRESH: -1
    CONV_DIMS:
    - -1
    HEAD_NAME: StandardRPNHead
    IN_FEATURES:
    - res4
    IOU_LABELS:
    - 0
    - -1
    - 1
    IOU_THRESHOLDS:
    - 0.3
    - 0.7
    LOSS_WEIGHT: 1.0
    NMS_THRESH: 0.7
    POSITIVE_FRACTION: 0.5
    POST_NMS_TOPK_TEST: 1000
    POST_NMS_TOPK_TRAIN: 2000
    PRE_NMS_TOPK_TEST: 6000
    PRE_NMS_TOPK_TRAIN: 12000
    SMOOTH_L1_BETA: 0.0
  SAN:
    ASYMETRIC_INPUT: true
    CLASS_WEIGHT: 2.0
    CLIP_DEEPER_FROZEN_EXCLUDE: []
    CLIP_FROZEN_EXCLUDE:
    - positional_embedding
    CLIP_MODEL_NAME: ViT-B/16
    CLIP_PRETRAINED_NAME: openai
    CLIP_RESOLUTION: 0.5
    CLIP_TEMPLATE_SET: vild
    DICE_WEIGHT: 5.0
    FEATURE_LAST_LAYER_IDX: 9
    IMPORTANCE_SAMPLE_RATIO: 0.75
    MASK_WEIGHT: 5.0
    NO_OBJECT_WEIGHT: 0.1
    NUM_CLASSES: 171
    OVERSAMPLE_RATIO: 3.0
    REC_CROSS_ATTN: false
    REC_DOWNSAMPLE_METHOD: max
    SEM_SEG_POSTPROCESS_BEFORE_INFERENCE: true
    SIZE_DIVISIBILITY: 32
    SLS_TOKEN_FORMAT: cls_token
    TRAIN_NUM_POINTS: 12544
    VQ_WEIGHT: 1.0
  SEM_SEG_HEAD:
    ASPP_CHANNELS: 256
    ASPP_DILATIONS:
    - 6
    - 12
    - 18
    ASPP_DROPOUT: 0.1
    COMMON_STRIDE: 4
    CONVS_DIM: 128
    IGNORE_VALUE: 255
    IN_FEATURES:
    - p2
    - p3
    - p4
    - p5
    LOSS_TYPE: hard_pixel_mining
    LOSS_WEIGHT: 1.0
    NAME: SemSegFPNHead
    NORM: GN
    NUM_CLASSES: 54
    PROJECT_CHANNELS:
    - 48
    PROJECT_FEATURES:
    - res2
    USE_DEPTHWISE_SEPARABLE_CONV: false
  SIDE_ADAPTER:
    ATTN_BIAS:
      EMBED_CHANNELS: 256
      MLP_CHANNELS: 256
      MLP_NUM_LAYERS: 3
      NUM_HEADS: 12
      NUM_LAYERS: 1
      RESCALE_ATTN_BIAS: true
    DEEP_SUPERVISION_IDXS:
    - 7
    - 8
    DROP_PATH_RATE: 0.0
    FUSION_MAP:
    - 0->0
    - 3->1
    - 6->2
    - 9->3
    FUSION_TYPE: vq
    IMAGE_SIZE: 640
    NAME: RegionwiseSideAdapterNetwork
    NUM_QUERIES: 1024
    PRETRAINED: false
    VIT_NAME: vit_w240n6d8_patch16
  WEIGHTS: ''
OUTPUT_DIR: output/vqsan1024_xattn
SEED: -1
SOLVER:
  AMP:
    ENABLED: true
  BACKBONE_MULTIPLIER: 1.0
  BASE_LR: 0.0001
  BASE_LR_END: 0.0
  BIAS_LR_FACTOR: 1.0
  CHECKPOINT_PERIOD: 5000
  CLIP_GRADIENTS:
    CLIP_TYPE: full_model
    CLIP_VALUE: 0.01
    ENABLED: true
    NORM_TYPE: 2.0
  CLIP_MULTIPLIER: 1.0
  GAMMA: 0.1
  IMS_PER_BATCH: 2
  LR_SCHEDULER_NAME: WarmupPolyLR
  MAX_ITER: 60000
  MOMENTUM: 0.9
  NESTEROV: false
  NUM_DECAYS: 3
  OPTIMIZER: ADAMW
  POLY_LR_CONSTANT_ENDING: 0.0
  POLY_LR_POWER: 0.9
  REFERENCE_WORLD_SIZE: 0
  RESCALE_INTERVAL: false
  STEPS:
  - 30000
  TEST_IMS_PER_BATCH: 1
  WARMUP_FACTOR: 1.0
  WARMUP_ITERS: 0
  WARMUP_METHOD: linear
  WEIGHT_DECAY: 0.0001
  WEIGHT_DECAY_BIAS: null
  WEIGHT_DECAY_EMBED: 0.0
  WEIGHT_DECAY_EMBED_GROUP:
  - absolute_pos_embed
  - positional_embedding
  - pos_embed
  - query_embed
  - relative_position_bias_table
  WEIGHT_DECAY_NORM: 0.0
TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 4480
    MIN_SIZES:
    - 320
    - 480
    - 640
    - 800
    - 960
    - 1120
  DETECTIONS_PER_IMAGE: 100
  EVAL_PERIOD: 5000
  EXPECTED_RESULTS: []
  KEYPOINT_OKS_SIGMAS: []
  PRECISE_BN:
    ENABLED: false
    NUM_ITER: 200
VERSION: 2
VIS_PERIOD: 0
WANDB:
  NAME: san_wandb_log
  PROJECT: oseg_1111

[12/02 00:32:18 detectron2]: Full config saved to output/vqsan1024_xattn/config.yaml
[12/02 00:32:18 d2.utils.env]: Using a generated random seed 19226282
wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin
wandb: - Waiting for wandb.init()...
wandb: \ Waiting for wandb.init()...
wandb: | Waiting for wandb.init()...
wandb: / Waiting for wandb.init()...
wandb: - Waiting for wandb.init()...
wandb: \ Waiting for wandb.init()...
wandb: | Waiting for wandb.init()...
wandb: wandb version 0.16.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.12
wandb: Run data is saved locally in /data/zjy/sources/SAN/wandb/run-20231202_003224-0cfonvyr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run san_wandb_log
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jayzz/oseg_1111
wandb: üöÄ View run at https://wandb.ai/jayzz/oseg_1111/runs/0cfonvyr
WARNING:timm.models._builder:No pretrained configuration specified for vit_tiny_patch16_224_in21k model. Using a default. Please add a config to the model pretrained_cfg registry or pass explicitly.
[12/02 00:32:43 d2.engine.defaults]: Model:
SAN(
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_vq': 1.0, 'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_vq_0': 1.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0}
      num_classes: 171
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (query_proj): Linear(in_features=240, out_features=768, bias=False)
  (side_adapter_network): RegionwiseSideAdapterNetwork(
    (vit_model): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 240, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (patch_drop): Identity()
      (norm_pre): Identity()
      (blocks): Sequential(
        (0): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (1): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (2): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (3): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (4): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (5): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (6): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (7): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
      )
      (fc_norm): Identity()
      (head_drop): Dropout(p=0.0, inplace=False)
      (head): Identity()
      (norm): Identity()
    )
    (fusion_layers): ModuleDict(
      (layer_0): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_1): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_2): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_3): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
    )
    (mask_decoder): MLPMaskDecoder(
      (query_mlp): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=240, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (pix_mlp): MLP(
        (layers): ModuleList(
          (0): Conv2d(240, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (attn_mlp): MLP(
        (layers): ModuleList(
          (0): Conv2d(240, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(256, 3072, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (bias_scaling): Linear(in_features=1, out_features=1, bias=True)
    )
  )
  (clip_visual_extractor): FeatureExtractor(
    (patchnorm_pre_ln): Identity()
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (patch_dropout): Identity()
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (3): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (4): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (5): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (6): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (7): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (8): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
    )
  )
  (clip_rec_head): RecWithAttnbiasHead(
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (ov_classifier): LearnableBgOvClassifier(
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (1): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (2): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (3): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (4): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (5): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (6): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (7): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (8): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (9): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (10): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (11): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (token_embedding): Embedding(49408, 512)
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)
[12/02 00:32:43 san]: Optimizer Info:
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                                     Param Name                                      |  Param Shape  |   Lr   |   Wd   |
+=====================================================================================+===============+========+========+
|                                  query_proj.weight                                  |    768X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                          side_adapter_network.query_embed                           |  1X1024X240   | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                        side_adapter_network.query_pos_embed                         |  1X1024X240   | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                      side_adapter_network.vit_model.pos_embed                       |  1X1600X240   | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.patch_embed.proj.weight                |  240X3X16X16  | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.patch_embed.proj.bias                 |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.0.norm1.weight                 |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                 side_adapter_network.vit_model.blocks.0.norm1.bias                  |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.0.attn.qkv.weight               |    720X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.0.attn.qkv.bias                |      720      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|              side_adapter_network.vit_model.blocks.0.attn.proj.weight               |    240X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.0.attn.proj.bias                |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.0.norm2.weight                 |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                 side_adapter_network.vit_model.blocks.0.norm2.bias                  |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.0.mlp.fc1.weight                |    960X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.0.mlp.fc1.bias                 |      960      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.0.mlp.fc2.weight                |    240X960    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.0.mlp.fc2.bias                 |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.1.norm1.weight                 |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                 side_adapter_network.vit_model.blocks.1.norm1.bias                  |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.1.attn.qkv.weight               |    720X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.1.attn.qkv.bias                |      720      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|              side_adapter_network.vit_model.blocks.1.attn.proj.weight               |    240X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.1.attn.proj.bias                |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.1.norm2.weight                 |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                 side_adapter_network.vit_model.blocks.1.norm2.bias                  |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.1.mlp.fc1.weight                |    960X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.1.mlp.fc1.bias                 |      960      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.1.mlp.fc2.weight                |    240X960    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.1.mlp.fc2.bias                 |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.2.norm1.weight                 |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                 side_adapter_network.vit_model.blocks.2.norm1.bias                  |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.2.attn.qkv.weight               |    720X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.2.attn.qkv.bias                |      720      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|              side_adapter_network.vit_model.blocks.2.attn.proj.weight               |    240X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.2.attn.proj.bias                |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.2.norm2.weight                 |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                 side_adapter_network.vit_model.blocks.2.norm2.bias                  |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.2.mlp.fc1.weight                |    960X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.2.mlp.fc1.bias                 |      960      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.2.mlp.fc2.weight                |    240X960    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.2.mlp.fc2.bias                 |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.3.norm1.weight                 |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                 side_adapter_network.vit_model.blocks.3.norm1.bias                  |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.3.attn.qkv.weight               |    720X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.3.attn.qkv.bias                |      720      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|              side_adapter_network.vit_model.blocks.3.attn.proj.weight               |    240X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.3.attn.proj.bias                |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.3.norm2.weight                 |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                 side_adapter_network.vit_model.blocks.3.norm2.bias                  |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.3.mlp.fc1.weight                |    960X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.3.mlp.fc1.bias                 |      960      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.3.mlp.fc2.weight                |    240X960    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.3.mlp.fc2.bias                 |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.4.norm1.weight                 |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                 side_adapter_network.vit_model.blocks.4.norm1.bias                  |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.4.attn.qkv.weight               |    720X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.4.attn.qkv.bias                |      720      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|              side_adapter_network.vit_model.blocks.4.attn.proj.weight               |    240X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.4.attn.proj.bias                |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.4.norm2.weight                 |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                 side_adapter_network.vit_model.blocks.4.norm2.bias                  |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.4.mlp.fc1.weight                |    960X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.4.mlp.fc1.bias                 |      960      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.4.mlp.fc2.weight                |    240X960    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.4.mlp.fc2.bias                 |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.5.norm1.weight                 |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                 side_adapter_network.vit_model.blocks.5.norm1.bias                  |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.5.attn.qkv.weight               |    720X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.5.attn.qkv.bias                |      720      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|              side_adapter_network.vit_model.blocks.5.attn.proj.weight               |    240X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.5.attn.proj.bias                |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.5.norm2.weight                 |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                 side_adapter_network.vit_model.blocks.5.norm2.bias                  |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.5.mlp.fc1.weight                |    960X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.5.mlp.fc1.bias                 |      960      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.5.mlp.fc2.weight                |    240X960    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.5.mlp.fc2.bias                 |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.6.norm1.weight                 |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                 side_adapter_network.vit_model.blocks.6.norm1.bias                  |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.6.attn.qkv.weight               |    720X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.6.attn.qkv.bias                |      720      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|              side_adapter_network.vit_model.blocks.6.attn.proj.weight               |    240X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.6.attn.proj.bias                |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.6.norm2.weight                 |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                 side_adapter_network.vit_model.blocks.6.norm2.bias                  |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.6.mlp.fc1.weight                |    960X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.6.mlp.fc1.bias                 |      960      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.6.mlp.fc2.weight                |    240X960    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.6.mlp.fc2.bias                 |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.7.norm1.weight                 |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                 side_adapter_network.vit_model.blocks.7.norm1.bias                  |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.7.attn.qkv.weight               |    720X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.7.attn.qkv.bias                |      720      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|              side_adapter_network.vit_model.blocks.7.attn.proj.weight               |    240X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.7.attn.proj.bias                |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.7.norm2.weight                 |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                 side_adapter_network.vit_model.blocks.7.norm2.bias                  |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.7.mlp.fc1.weight                |    960X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.7.mlp.fc1.bias                 |      960      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.vit_model.blocks.7.mlp.fc2.weight                |    240X960    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.vit_model.blocks.7.mlp.fc2.bias                 |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|           side_adapter_network.fusion_layers.layer_0.input_proj.0.weight            |      768      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|            side_adapter_network.fusion_layers.layer_0.input_proj.0.bias             |      768      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|           side_adapter_network.fusion_layers.layer_0.input_proj.1.weight            |  240X768X1X1  | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|            side_adapter_network.fusion_layers.layer_0.input_proj.1.bias             |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|    side_adapter_network.fusion_layers.layer_0.cross_attn.layers.0.0.norm.weight     |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|     side_adapter_network.fusion_layers.layer_0.cross_attn.layers.0.0.norm.bias      |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_0.cross_attn.layers.0.0.fn.query.weight   |    480X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|   side_adapter_network.fusion_layers.layer_0.cross_attn.layers.0.0.fn.key.weight    |    480X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_0.cross_attn.layers.0.0.fn.value.weight   |    480X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
| side_adapter_network.fusion_layers.layer_0.cross_attn.layers.0.0.fn.to_out.0.weight |    240X480    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_0.cross_attn.layers.0.0.fn.to_out.0.bias  |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|    side_adapter_network.fusion_layers.layer_0.cross_attn.layers.0.1.norm.weight     |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|     side_adapter_network.fusion_layers.layer_0.cross_attn.layers.0.1.norm.bias      |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_0.cross_attn.layers.0.1.fn.query.weight   |    480X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|   side_adapter_network.fusion_layers.layer_0.cross_attn.layers.0.1.fn.key.weight    |    480X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_0.cross_attn.layers.0.1.fn.value.weight   |    480X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
| side_adapter_network.fusion_layers.layer_0.cross_attn.layers.0.1.fn.to_out.0.weight |    240X480    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_0.cross_attn.layers.0.1.fn.to_out.0.bias  |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|    side_adapter_network.fusion_layers.layer_0.cross_attn.layers.0.2.norm.weight     |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|     side_adapter_network.fusion_layers.layer_0.cross_attn.layers.0.2.norm.bias      |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_0.cross_attn.layers.0.2.fn.net.0.weight   |    640X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|   side_adapter_network.fusion_layers.layer_0.cross_attn.layers.0.2.fn.net.0.bias    |      640      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_0.cross_attn.layers.0.2.fn.net.3.weight   |    240X640    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|   side_adapter_network.fusion_layers.layer_0.cross_attn.layers.0.2.fn.net.3.bias    |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|           side_adapter_network.fusion_layers.layer_1.input_proj.0.weight            |      768      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|            side_adapter_network.fusion_layers.layer_1.input_proj.0.bias             |      768      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|           side_adapter_network.fusion_layers.layer_1.input_proj.1.weight            |  240X768X1X1  | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|            side_adapter_network.fusion_layers.layer_1.input_proj.1.bias             |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|    side_adapter_network.fusion_layers.layer_1.cross_attn.layers.0.0.norm.weight     |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|     side_adapter_network.fusion_layers.layer_1.cross_attn.layers.0.0.norm.bias      |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_1.cross_attn.layers.0.0.fn.query.weight   |    480X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|   side_adapter_network.fusion_layers.layer_1.cross_attn.layers.0.0.fn.key.weight    |    480X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_1.cross_attn.layers.0.0.fn.value.weight   |    480X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
| side_adapter_network.fusion_layers.layer_1.cross_attn.layers.0.0.fn.to_out.0.weight |    240X480    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_1.cross_attn.layers.0.0.fn.to_out.0.bias  |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|    side_adapter_network.fusion_layers.layer_1.cross_attn.layers.0.1.norm.weight     |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|     side_adapter_network.fusion_layers.layer_1.cross_attn.layers.0.1.norm.bias      |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_1.cross_attn.layers.0.1.fn.query.weight   |    480X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|   side_adapter_network.fusion_layers.layer_1.cross_attn.layers.0.1.fn.key.weight    |    480X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_1.cross_attn.layers.0.1.fn.value.weight   |    480X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
| side_adapter_network.fusion_layers.layer_1.cross_attn.layers.0.1.fn.to_out.0.weight |    240X480    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_1.cross_attn.layers.0.1.fn.to_out.0.bias  |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|    side_adapter_network.fusion_layers.layer_1.cross_attn.layers.0.2.norm.weight     |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|     side_adapter_network.fusion_layers.layer_1.cross_attn.layers.0.2.norm.bias      |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_1.cross_attn.layers.0.2.fn.net.0.weight   |    640X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|   side_adapter_network.fusion_layers.layer_1.cross_attn.layers.0.2.fn.net.0.bias    |      640      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_1.cross_attn.layers.0.2.fn.net.3.weight   |    240X640    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|   side_adapter_network.fusion_layers.layer_1.cross_attn.layers.0.2.fn.net.3.bias    |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|           side_adapter_network.fusion_layers.layer_2.input_proj.0.weight            |      768      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|            side_adapter_network.fusion_layers.layer_2.input_proj.0.bias             |      768      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|           side_adapter_network.fusion_layers.layer_2.input_proj.1.weight            |  240X768X1X1  | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|            side_adapter_network.fusion_layers.layer_2.input_proj.1.bias             |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|    side_adapter_network.fusion_layers.layer_2.cross_attn.layers.0.0.norm.weight     |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|     side_adapter_network.fusion_layers.layer_2.cross_attn.layers.0.0.norm.bias      |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_2.cross_attn.layers.0.0.fn.query.weight   |    480X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|   side_adapter_network.fusion_layers.layer_2.cross_attn.layers.0.0.fn.key.weight    |    480X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_2.cross_attn.layers.0.0.fn.value.weight   |    480X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
| side_adapter_network.fusion_layers.layer_2.cross_attn.layers.0.0.fn.to_out.0.weight |    240X480    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_2.cross_attn.layers.0.0.fn.to_out.0.bias  |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|    side_adapter_network.fusion_layers.layer_2.cross_attn.layers.0.1.norm.weight     |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|     side_adapter_network.fusion_layers.layer_2.cross_attn.layers.0.1.norm.bias      |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_2.cross_attn.layers.0.1.fn.query.weight   |    480X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|   side_adapter_network.fusion_layers.layer_2.cross_attn.layers.0.1.fn.key.weight    |    480X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_2.cross_attn.layers.0.1.fn.value.weight   |    480X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
| side_adapter_network.fusion_layers.layer_2.cross_attn.layers.0.1.fn.to_out.0.weight |    240X480    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_2.cross_attn.layers.0.1.fn.to_out.0.bias  |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|    side_adapter_network.fusion_layers.layer_2.cross_attn.layers.0.2.norm.weight     |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|     side_adapter_network.fusion_layers.layer_2.cross_attn.layers.0.2.norm.bias      |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_2.cross_attn.layers.0.2.fn.net.0.weight   |    640X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|   side_adapter_network.fusion_layers.layer_2.cross_attn.layers.0.2.fn.net.0.bias    |      640      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_2.cross_attn.layers.0.2.fn.net.3.weight   |    240X640    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|   side_adapter_network.fusion_layers.layer_2.cross_attn.layers.0.2.fn.net.3.bias    |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|           side_adapter_network.fusion_layers.layer_3.input_proj.0.weight            |      768      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|            side_adapter_network.fusion_layers.layer_3.input_proj.0.bias             |      768      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|           side_adapter_network.fusion_layers.layer_3.input_proj.1.weight            |  240X768X1X1  | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|            side_adapter_network.fusion_layers.layer_3.input_proj.1.bias             |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|    side_adapter_network.fusion_layers.layer_3.cross_attn.layers.0.0.norm.weight     |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|     side_adapter_network.fusion_layers.layer_3.cross_attn.layers.0.0.norm.bias      |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_3.cross_attn.layers.0.0.fn.query.weight   |    480X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|   side_adapter_network.fusion_layers.layer_3.cross_attn.layers.0.0.fn.key.weight    |    480X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_3.cross_attn.layers.0.0.fn.value.weight   |    480X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
| side_adapter_network.fusion_layers.layer_3.cross_attn.layers.0.0.fn.to_out.0.weight |    240X480    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_3.cross_attn.layers.0.0.fn.to_out.0.bias  |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|    side_adapter_network.fusion_layers.layer_3.cross_attn.layers.0.1.norm.weight     |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|     side_adapter_network.fusion_layers.layer_3.cross_attn.layers.0.1.norm.bias      |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_3.cross_attn.layers.0.1.fn.query.weight   |    480X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|   side_adapter_network.fusion_layers.layer_3.cross_attn.layers.0.1.fn.key.weight    |    480X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_3.cross_attn.layers.0.1.fn.value.weight   |    480X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
| side_adapter_network.fusion_layers.layer_3.cross_attn.layers.0.1.fn.to_out.0.weight |    240X480    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_3.cross_attn.layers.0.1.fn.to_out.0.bias  |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|    side_adapter_network.fusion_layers.layer_3.cross_attn.layers.0.2.norm.weight     |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|     side_adapter_network.fusion_layers.layer_3.cross_attn.layers.0.2.norm.bias      |      240      | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_3.cross_attn.layers.0.2.fn.net.0.weight   |    640X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|   side_adapter_network.fusion_layers.layer_3.cross_attn.layers.0.2.fn.net.0.bias    |      640      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|  side_adapter_network.fusion_layers.layer_3.cross_attn.layers.0.2.fn.net.3.weight   |    240X640    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|   side_adapter_network.fusion_layers.layer_3.cross_attn.layers.0.2.fn.net.3.bias    |      240      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|             side_adapter_network.mask_decoder.query_mlp.layers.0.weight             |    256X240    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|              side_adapter_network.mask_decoder.query_mlp.layers.0.bias              |      256      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|             side_adapter_network.mask_decoder.query_mlp.layers.1.weight             |    256X256    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|              side_adapter_network.mask_decoder.query_mlp.layers.1.bias              |      256      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|             side_adapter_network.mask_decoder.query_mlp.layers.2.weight             |    256X256    | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|              side_adapter_network.mask_decoder.query_mlp.layers.2.bias              |      256      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|              side_adapter_network.mask_decoder.pix_mlp.layers.0.weight              |  256X240X1X1  | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.mask_decoder.pix_mlp.layers.0.bias               |      256      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|              side_adapter_network.mask_decoder.pix_mlp.layers.1.weight              |  256X256X1X1  | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.mask_decoder.pix_mlp.layers.1.bias               |      256      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|              side_adapter_network.mask_decoder.pix_mlp.layers.2.weight              |  256X256X1X1  | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|               side_adapter_network.mask_decoder.pix_mlp.layers.2.bias               |      256      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|             side_adapter_network.mask_decoder.attn_mlp.layers.0.weight              |  256X240X1X1  | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|              side_adapter_network.mask_decoder.attn_mlp.layers.0.bias               |      256      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|             side_adapter_network.mask_decoder.attn_mlp.layers.1.weight              |  256X256X1X1  | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|              side_adapter_network.mask_decoder.attn_mlp.layers.1.bias               |      256      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|             side_adapter_network.mask_decoder.attn_mlp.layers.2.weight              | 3072X256X1X1  | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|              side_adapter_network.mask_decoder.attn_mlp.layers.2.bias               |     3072      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                side_adapter_network.mask_decoder.bias_scaling.weight                |      1X1      | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                 side_adapter_network.mask_decoder.bias_scaling.bias                 |       1       | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                     clip_visual_extractor.positional_embedding                      |    197X768    | 0.0001 |  0.0   |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                               ov_classifier.bg_embed                                |     1X512     | 0.0001 | 0.0001 |
+-------------------------------------------------------------------------------------+---------------+--------+--------+
|                                        Total                                        |    13.93M     |   -    |   -    |
+-------------------------------------------------------------------------------------+---------------+--------+--------+

[12/02 00:32:43 san.data.dataset_mappers.mask_former_semantic_dataset_mapper]: [MaskFormerSemanticDatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(320, 384, 448, 512, 576, 640, 704, 768, 832, 896, 960), max_size=2560, sample_style='choice'), RandomCrop_CategoryAreaConstraint(crop_type='absolute', crop_size=[640, 640], single_category_max_area=1.0, ignored_category=255), <detectron2.projects.point_rend.color_augmentation.ColorAugSSDTransform object at 0x7f8d5c74f1c0>, RandomFlip()]
[12/02 00:32:43 d2.data.datasets.coco]: Loaded 9000 images with semantic segmentation from datasets/coco/coco_stuff_10k/images_detectron2/train
[12/02 00:32:43 san.data.build]: Using training sampler TrainingSampler
[12/02 00:32:43 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[12/02 00:32:43 d2.data.common]: Serializing 9000 elements to byte tensors and concatenating them all ...
[12/02 00:32:43 d2.data.common]: Serialized dataset takes 2.47 MiB
[12/02 00:32:43 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from  ...
[12/02 00:32:43 fvcore.common.checkpoint]: No checkpoint found. Initializing model from scratch
[12/02 00:32:43 d2.engine.train_loop]: Starting training from iteration 0
[12/02 00:32:47 san.model.side_adapter.side_adapter]: fuse clip 0 to 0
[12/02 00:32:47 san.model.side_adapter.side_adapter]: fuse clip 3 to 1
[12/02 00:32:47 san.model.side_adapter.side_adapter]: fuse clip 6 to 2
[12/02 00:32:47 san.model.side_adapter.side_adapter]: fuse clip 9 to 3
/home/zjy/.conda/envs/san/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
[12/02 00:32:58 d2.utils.events]:  eta: 4:55:17  iter: 19  total_loss: 134.4  loss_ce: 56.18  loss_mask: 3.134  loss_dice: 4.201  loss_ce_0: 56.18  loss_mask_0: 3.161  loss_dice_0: 4.206  loss_vq: 3.601  loss_vq_0: 3.601    time: 0.4294  last_time: 0.2875  data_time: 0.0173  last_data_time: 0.0048   lr: 9.9971e-05  max_mem: 7607M
[12/02 00:33:04 d2.utils.events]:  eta: 4:54:44  iter: 39  total_loss: 103.3  loss_ce: 41.7  loss_mask: 1.927  loss_dice: 4.493  loss_ce_0: 41.71  loss_mask_0: 1.942  loss_dice_0: 4.475  loss_vq: 3.42  loss_vq_0: 3.42    time: 0.3667  last_time: 0.2899  data_time: 0.0056  last_data_time: 0.0055   lr: 9.9941e-05  max_mem: 7612M
[12/02 00:33:10 d2.utils.events]:  eta: 4:52:41  iter: 59  total_loss: 57.69  loss_ce: 19.57  loss_mask: 1.95  loss_dice: 4.188  loss_ce_0: 19.58  loss_mask_0: 2  loss_dice_0: 4.207  loss_vq: 3.207  loss_vq_0: 3.207    time: 0.3400  last_time: 0.2937  data_time: 0.0050  last_data_time: 0.0041   lr: 9.9911e-05  max_mem: 7612M
[12/02 00:33:16 d2.utils.events]:  eta: 4:51:55  iter: 79  total_loss: 20.14  loss_ce: 1.267  loss_mask: 1.747  loss_dice: 4.135  loss_ce_0: 1.215  loss_mask_0: 1.822  loss_dice_0: 4.162  loss_vq: 2.969  loss_vq_0: 2.969    time: 0.3273  last_time: 0.2897  data_time: 0.0055  last_data_time: 0.0047   lr: 9.9881e-05  max_mem: 7612M
[12/02 00:33:22 d2.utils.events]:  eta: 4:50:31  iter: 99  total_loss: 19.14  loss_ce: 1.077  loss_mask: 1.943  loss_dice: 3.89  loss_ce_0: 0.9766  loss_mask_0: 1.939  loss_dice_0: 3.881  loss_vq: 2.562  loss_vq_0: 2.562    time: 0.3199  last_time: 0.2965  data_time: 0.0050  last_data_time: 0.0074   lr: 9.9851e-05  max_mem: 7612M
[12/02 00:33:28 d2.utils.events]:  eta: 4:50:41  iter: 119  total_loss: 17.85  loss_ce: 1.057  loss_mask: 1.519  loss_dice: 3.926  loss_ce_0: 1.031  loss_mask_0: 1.525  loss_dice_0: 3.922  loss_vq: 2.323  loss_vq_0: 2.323    time: 0.3155  last_time: 0.2899  data_time: 0.0055  last_data_time: 0.0048   lr: 9.9821e-05  max_mem: 7612M
[12/02 00:33:35 d2.utils.events]:  eta: 4:50:51  iter: 139  total_loss: 17.85  loss_ce: 0.8892  loss_mask: 1.893  loss_dice: 3.834  loss_ce_0: 0.8502  loss_mask_0: 1.923  loss_dice_0: 3.901  loss_vq: 2.287  loss_vq_0: 2.287    time: 0.3145  last_time: 0.3136  data_time: 0.0054  last_data_time: 0.0086   lr: 9.9791e-05  max_mem: 7614M
[12/02 00:33:41 d2.utils.events]:  eta: 4:50:45  iter: 159  total_loss: 16.78  loss_ce: 0.8534  loss_mask: 1.806  loss_dice: 3.686  loss_ce_0: 0.8142  loss_mask_0: 1.759  loss_dice_0: 3.784  loss_vq: 2.149  loss_vq_0: 2.149    time: 0.3127  last_time: 0.3237  data_time: 0.0054  last_data_time: 0.0031   lr: 9.9761e-05  max_mem: 7614M
[12/02 00:33:47 d2.utils.events]:  eta: 4:50:45  iter: 179  total_loss: 16.93  loss_ce: 0.9003  loss_mask: 1.669  loss_dice: 3.932  loss_ce_0: 0.7916  loss_mask_0: 1.776  loss_dice_0: 3.945  loss_vq: 2.025  loss_vq_0: 2.025    time: 0.3106  last_time: 0.2942  data_time: 0.0053  last_data_time: 0.0052   lr: 9.9731e-05  max_mem: 7614M
[12/02 00:33:53 d2.utils.events]:  eta: 4:50:30  iter: 199  total_loss: 16.73  loss_ce: 0.7723  loss_mask: 1.779  loss_dice: 3.66  loss_ce_0: 0.7644  loss_mask_0: 1.808  loss_dice_0: 3.75  loss_vq: 2.029  loss_vq_0: 2.029    time: 0.3088  last_time: 0.2951  data_time: 0.0054  last_data_time: 0.0060   lr: 9.9701e-05  max_mem: 7614M
[12/02 00:33:59 d2.utils.events]:  eta: 4:50:16  iter: 219  total_loss: 16.79  loss_ce: 1.055  loss_mask: 1.722  loss_dice: 3.759  loss_ce_0: 0.9775  loss_mask_0: 1.703  loss_dice_0: 3.784  loss_vq: 1.956  loss_vq_0: 1.956    time: 0.3074  last_time: 0.2892  data_time: 0.0052  last_data_time: 0.0060   lr: 9.9671e-05  max_mem: 7614M
[12/02 00:34:05 d2.utils.events]:  eta: 4:49:57  iter: 239  total_loss: 16.53  loss_ce: 0.8934  loss_mask: 1.725  loss_dice: 3.593  loss_ce_0: 0.8556  loss_mask_0: 1.626  loss_dwandb: Network error (TransientError), entering retry loop.
  last_time: 0.2912  data_time: 0.0051  last_data_time: 0.0059   lr: 9.9641e-05  max_mem: 7614M
[12/02 00:34:11 d2.utils.events]:  eta: 4:49:56  iter: 259  total_loss: 16.08  loss_ce: 0.962  loss_mask: 1.586  loss_dice: 3.683  loss_ce_0: 0.9425  loss_mask_0: 1.705  loss_dice_0: 3.812  loss_vq: 1.81  loss_vq_0: 1.81    time: 0.3049  last_time: 0.2964  data_time: 0.0057  last_data_time: 0.0064   lr: 9.9611e-05  max_mem: 7616M
[12/02 00:34:17 d2.utils.events]:  eta: 4:49:32  iter: 279  total_loss: 16.62  loss_ce: 1.013  loss_mask: 2.008  loss_dice: 3.462  loss_ce_0: 1.001  loss_mask_0: 2.04  loss_dice_0: 3.483  loss_vq: 1.807  loss_vq_0: 1.807    time: 0.3039  last_time: 0.2888  data_time: 0.0052  last_data_time: 0.0016   lr: 9.9581e-05  max_mem: 7616M
[12/02 00:34:23 d2.utils.events]:  eta: 4:49:36  iter: 299  total_loss: 16.52  loss_ce: 0.8311  loss_mask: 1.736  loss_dice: 3.637  loss_ce_0: 0.7864  loss_mask_0: 1.801  loss_dice_0: 3.744  loss_vq: 1.823  loss_vq_0: 1.823    time: 0.3041  last_time: 0.2946  data_time: 0.0055  last_data_time: 0.0075   lr: 9.9551e-05  max_mem: 7618M
[12/02 00:34:29 d2.utils.events]:  eta: 4:49:19  iter: 319  total_loss: 16.47  loss_ce: 0.8536  loss_mask: 1.678  loss_dice: 3.629  loss_ce_0: 0.879  loss_mask_0: 1.695  loss_dice_0: 3.68  loss_vq: 1.756  loss_vq_0: 1.756    time: 0.3032  last_time: 0.2882  data_time: 0.0052  last_data_time: 0.0052   lr: 9.9521e-05  max_mem: 7618M
[12/02 00:34:35 d2.utils.events]:  eta: 4:49:10  iter: 339  total_loss: 16.59  loss_ce: 0.9014  loss_mask: 1.586  loss_dice: 3.879  loss_ce_0: 0.8972  loss_mask_0: 1.62  loss_dice_0: 3.924  loss_vq: 1.739  loss_vq_0: 1.739    time: 0.3024  last_time: 0.2913  data_time: 0.0052  last_data_time: 0.0051   lr: 9.9491e-05  max_mem: 7618M
[12/02 00:34:41 d2.utils.events]:  eta: 4:49:07  iter: 359  total_loss: 15.17  loss_ce: 0.7589  loss_mask: 1.5  loss_dice: 3.531  loss_ce_0: 0.7282  loss_mask_0: 1.529  loss_dice_0: 3.571  loss_vq: 1.708  loss_vq_0: 1.708    time: 0.3019  last_time: 0.2942  data_time: 0.0054  last_data_time: 0.0044   lr: 9.9461e-05  max_mem: 7618M
[12/02 00:34:47 d2.utils.events]:  eta: 4:48:59  iter: 379  total_loss: 16.05  loss_ce: 0.6137  loss_mask: 1.962  loss_dice: 3.727  loss_ce_0: 0.6148  loss_mask_0: 1.941  loss_dice_0: 3.863  loss_vq: 1.703  loss_vq_0: 1.703    time: 0.3014  last_time: 0.3009  data_time: 0.0052  last_data_time: 0.0073   lr: 9.9431e-05  max_mem: 7618M
[12/02 00:34:53 d2.utils.events]:  eta: 4:49:00  iter: 399  total_loss: 15.93  loss_ce: 0.8383  loss_mask: 1.769  loss_dice: 3.609  loss_ce_0: 0.7684  loss_mask_0: 1.803  loss_dice_0: 3.671  loss_vq: 1.636  loss_vq_0: 1.636    time: 0.3009  last_time: 0.2862  data_time: 0.0053  last_data_time: 0.0045   lr: 9.9401e-05  max_mem: 7618M
[12/02 00:34:59 d2.utils.events]:  eta: 4:48:54  iter: 419  total_loss: 15.92  loss_ce: 0.8714  loss_mask: 1.777  loss_dice: 3.632  loss_ce_0: 0.827  loss_mask_0: 1.783  loss_dice_0: 3.68  loss_vq: 1.62  loss_vq_0: 1.62    time: 0.3005  last_time: 0.2874  data_time: 0.0055  last_data_time: 0.0032   lr: 9.9371e-05  max_mem: 7618M
[12/02 00:35:05 d2.utils.events]:  eta: 4:48:42  iter: 439  total_loss: 15.84  loss_ce: 0.7175  loss_mask: 1.81  loss_dice: 3.752  loss_ce_0: 0.7003  loss_mask_0: 1.809  loss_dice_0: 3.811  loss_vq: 1.56  loss_vq_0: 1.56    time: 0.3000  last_time: 0.2899  data_time: 0.0051  last_data_time: 0.0045   lr: 9.9341e-05  max_mem: 7618M
[12/02 00:35:11 d2.utils.events]:  eta: 4:48:43  iter: 459  total_loss: 15.7  loss_ce: 1.029  loss_mask: 1.759  loss_dice: 3.51  loss_ce_0: 1.003  loss_mask_0: 1.728  loss_dice_0: 3.596  loss_vq: 1.508  loss_vq_0: 1.508    time: 0.3004  last_time: 0.3099  data_time: 0.0053  last_data_time: 0.0077   lr: 9.9311e-05  max_mem: 7621M
[12/02 00:35:17 d2.utils.events]:  eta: 4:48:44  iter: 479  total_loss: 15.63  loss_ce: 0.9243  loss_mask: 1.629  loss_dice: 3.563  loss_ce_0: 0.8655  loss_mask_0: 1.585  loss_dice_0: 3.581  loss_vq: 1.523  loss_vq_0: 1.523    time: 0.3001  last_time: 0.2913  data_time: 0.0052  last_data_time: 0.0048   lr: 9.9281e-05  max_mem: 7621M
[12/02 00:35:23 d2.utils.events]:  eta: 4:48:40  iter: 499  total_loss: 15.23  loss_ce: 0.8132  loss_mask: 1.744  loss_dice: 3.591  loss_ce_0: 0.7695  loss_mask_0: 1.698  loss_dice_0: 3.656  loss_vq: 1.467  loss_vq_0: 1.467    time: 0.3000  last_time: 0.3125  data_time: 0.0054  last_data_time: 0.0071   lr: 9.9251e-05  max_mem: 7621M
[12/02 00:35:29 d2.utils.events]:  eta: 4:48:38  iter: 519  total_loss: 15.92  loss_ce: 0.7068  loss_mask: 1.814  loss_dice: 3.658  loss_ce_0: 0.6834  loss_mask_0: 1.785  loss_dice_0: 3.724  loss_vq: 1.62  loss_vq_0: 1.62    time: 0.2997  last_time: 0.2883  data_time: 0.0060  last_data_time: 0.0066   lr: 9.9221e-05  max_mem: 7621M
[12/02 00:35:35 d2.utils.events]:  eta: 4:48:28  iter: 539  total_loss: 14.87  loss_ce: 0.6553  loss_mask: 1.968  loss_dice: 3.246  loss_ce_0: 0.6485  loss_mask_0: 1.983  loss_dice_0: 3.248  loss_vq: 1.496  loss_vq_0: 1.496    time: 0.2995  last_time: 0.2894  data_time: 0.0054  last_data_time: 0.0055   lr: 9.9191e-05  max_mem: 7621M
[12/02 00:35:41 d2.utils.events]:  eta: 4:48:21  iter: 559  total_loss: 15.22  loss_ce: 0.6898  loss_mask: 1.804  loss_dice: 3.707  loss_ce_0: 0.6826  loss_mask_0: 1.676  loss_dice_0: 3.786  loss_vq: 1.451  loss_vq_0: 1.451    time: 0.2993  last_time: 0.2908  data_time: 0.0052  last_data_time: 0.0053   lr: 9.9161e-05  max_mem: 7621M
[12/02 00:35:47 d2.utils.events]:  eta: 4:48:17  iter: 579  total_loss: 14.92  loss_ce: 0.7219  loss_mask: 1.835  loss_dice: 3.413  loss_ce_0: 0.6939  loss_mask_0: 1.84  loss_dice_0: 3.456  loss_vq: 1.441  loss_vq_0: 1.441    time: 0.2990  last_time: 0.2875  data_time: 0.0047  last_data_time: 0.0049   lr: 9.9131e-05  max_mem: 7621M
[12/02 00:35:53 d2.utils.events]:  eta: 4:48:11  iter: 599  total_loss: 14.46  loss_ce: 0.6276  loss_mask: 1.82  loss_dice: 3.31  loss_ce_0: 0.6171  loss_mask_0: 1.85  loss_dice_0: 3.368  loss_vq: 1.426  loss_vq_0: 1.426    time: 0.2988  last_time: 0.2874  data_time: 0.0051  last_data_time: 0.0043   lr: 9.9101e-05  max_mem: 7621M
[12/02 00:35:59 d2.utils.events]:  eta: 4:48:03  iter: 619  total_loss: 14.36  loss_ce: 0.6802  loss_mask: 1.898  loss_dice: 3.365  loss_ce_0: 0.6876  loss_mask_0: 1.834  loss_dice_0: 3.421  loss_vq: 1.427  loss_vq_0: 1.427    time: 0.2986  last_time: 0.2909  data_time: 0.0052  last_data_time: 0.0055   lr: 9.9071e-05  max_mem: 7621M
[12/02 00:36:05 d2.utils.events]:  eta: 4:47:56  iter: 639  total_loss: 14.94  loss_ce: 0.7091  loss_mask: 1.792  loss_dice: 3.389  loss_ce_0: 0.6944  loss_mask_0: 1.809  loss_dice_0: 3.502  loss_vq: 1.44  loss_vq_0: 1.44    time: 0.2984  last_time: 0.2926  data_time: 0.0053  last_data_time: 0.0072   lr: 9.9041e-05  max_mem: 7621M
[12/02 00:36:11 d2.utils.events]:  eta: 4:47:53  iter: 659  total_loss: 13.99  loss_ce: 0.6649  loss_mask: 1.667  loss_dice: 3.328  loss_ce_0: 0.6756  loss_mask_0: 1.647  loss_dice_0: 3.361  loss_vq: 1.422  loss_vq_0: 1.422    time: 0.2984  last_time: 0.2845  data_time: 0.0050  last_data_time: 0.0015   lr: 9.9011e-05  max_mem: 7621M
[12/02 00:36:17 d2.utils.events]:  eta: 4:47:50  iter: 679  total_loss: 14.28  loss_ce: 0.5676  loss_mask: 1.848  loss_dice: 3.334  loss_ce_0: 0.5574  loss_mask_0: 1.858  loss_dice_0: 3.373  loss_vq: 1.43  loss_vq_0: 1.43    time: 0.2982  last_time: 0.2999  data_time: 0.0056  last_data_time: 0.0102   lr: 9.8981e-05  max_mem: 7621M
[12/02 00:36:23 d2.utils.events]:  eta: 4:47:46  iter: 699  total_loss: 14.8  loss_ce: 0.6378  loss_mask: 1.969  loss_dice: 3.428  loss_ce_0: 0.6312  loss_mask_0: 1.955  loss_dice_0: 3.505  loss_vq: 1.363  loss_vq_0: 1.363    time: 0.2981  last_time: 0.2917  data_time: 0.0051  last_data_time: 0.0059   lr: 9.8951e-05  max_mem: 7621M
[12/02 00:36:29 d2.utils.events]:  eta: 4:47:40  iter: 719  total_loss: 14  loss_ce: 0.5872  loss_mask: 1.653  loss_dice: 3.405  loss_ce_0: 0.5753  loss_mask_0: 1.739  loss_dice_0: 3.344  loss_vq: 1.28  loss_vq_0: 1.28    time: 0.2979  last_time: 0.2936  data_time: 0.0050  last_data_time: 0.0078   lr: 9.8921e-05  max_mem: 7621M
[12/02 00:36:35 d2.utils.events]:  eta: 4:47:37  iter: 739  total_loss: 14.82  loss_ce: 0.64  loss_mask: 1.895  loss_dice: 3.567  loss_ce_0: 0.6627  loss_mask_0: 1.984  loss_dice_0: 3.546  loss_vq: 1.352  loss_vq_0: 1.352    time: 0.2978  last_time: 0.2879  data_time: 0.0055  last_data_time: 0.0045   lr: 9.8891e-05  max_mem: 7621M
[12/02 00:36:41 d2.utils.events]:  eta: 4:47:33  iter: 759  total_loss: 14.61  loss_ce: 0.6005  loss_mask: 2.182  loss_dice: 3.304  loss_ce_0: 0.5809  loss_mask_0: 2.164  loss_dice_0: 3.278  loss_vq: 1.318  loss_vq_0: 1.318    time: 0.2977  last_time: 0.3112  data_time: 0.0056  last_data_time: 0.0059   lr: 9.8861e-05  max_mem: 7621M
[12/02 00:36:47 d2.utils.events]:  eta: 4:47:29  iter: 779  total_loss: 14.31  loss_ce: 0.6156  loss_mask: 1.825  loss_dice: 3.458  loss_ce_0: 0.5819  loss_mask_0: 1.888  loss_dice_0: 3.404  loss_vq: 1.301  loss_vq_0: 1.301    time: 0.2977  last_time: 0.3391  data_time: 0.0056  last_data_time: 0.0141   lr: 9.8831e-05  max_mem: 7621M
[12/02 00:36:53 d2.utils.events]:  eta: 4:47:21  iter: 799  total_loss: 14.85  loss_ce: 0.5971  loss_mask: 2  loss_dice: 3.313  loss_ce_0: 0.5781  loss_mask_0: 2.017  loss_dice_0: 3.273  loss_vq: 1.381  loss_vq_0: 1.381    time: 0.2975  last_time: 0.2897  data_time: 0.0049  last_data_time: 0.0064   lr: 9.8801e-05  max_mem: 7621M
[12/02 00:36:59 d2.utils.events]:  eta: 4:47:15  iter: 819  total_loss: 13.73  loss_ce: 0.599  loss_mask: 1.665  loss_dice: 3.29  loss_ce_0: 0.5794  loss_mask_0: 1.636  loss_dice_0: 3.302  loss_vq: 1.342  loss_vq_0: 1.342    time: 0.2974  last_time: 0.2901  data_time: 0.0053  last_data_time: 0.0050   lr: 9.8771e-05  max_mem: 7621M
[12/02 00:37:05 d2.utils.events]:  eta: 4:47:10  iter: 839  total_loss: 14.26  loss_ce: 0.6371  loss_mask: 1.74  loss_dice: 3.486  loss_ce_0: 0.6067  loss_mask_0: 1.791  loss_dice_0: 3.568  loss_vq: 1.253  loss_vq_0: 1.253    time: 0.2973  last_time: 0.2930  data_time: 0.0055  last_data_time: 0.0065   lr: 9.8741e-05  max_mem: 7621M
[12/02 00:37:11 d2.utils.events]:  eta: 4:47:05  iter: 859  total_loss: 14.65  loss_ce: 0.709  loss_mask: 1.747  loss_dice: 3.55  loss_ce_0: 0.6743  loss_mask_0: 1.741  loss_dice_0: 3.564  loss_vq: 1.353  loss_vq_0: 1.353    time: 0.2972  last_time: 0.2874  data_time: 0.0053  last_data_time: 0.0036   lr: 9.8711e-05  max_mem: 7621M
[12/02 00:37:17 d2.utils.events]:  eta: 4:47:00  iter: 879  total_loss: 14.3  loss_ce: 0.6929  loss_mask: 1.688  loss_dice: 3.439  loss_ce_0: 0.6388  loss_mask_0: 1.72  loss_dice_0: 3.49  loss_vq: 1.278  loss_vq_0: 1.278    time: 0.2971  last_time: 0.2880  data_time: 0.0051  last_data_time: 0.0045   lr: 9.8681e-05  max_mem: 7621M
[12/02 00:37:23 d2.utils.events]:  eta: 4:46:56  iter: 899  total_loss: 14.1  loss_ce: 0.6159  loss_mask: 1.872  loss_dice: 3.113  loss_ce_0: 0.6346  loss_mask_0: 1.848  loss_dice_0: 3.19  loss_vq: 1.305  loss_vq_0: 1.305    time: 0.2970  last_time: 0.2876  data_time: 0.0049  last_data_time: 0.0042   lr: 9.865e-05  max_mem: 7621M
[12/02 00:37:29 d2.utils.events]:  eta: 4:46:51  iter: 919  total_loss: 13.84  loss_ce: 0.6844  loss_mask: 1.675  loss_dice: 3.253  loss_ce_0: 0.6523  loss_mask_0: 1.684  loss_dice_0: 3.352  loss_vq: 1.237  loss_vq_0: 1.237    time: 0.2969  last_time: 0.2938  data_time: 0.0051  last_data_time: 0.0056   lr: 9.862e-05  max_mem: 7621M
[12/02 00:37:35 d2.utils.events]:  eta: 4:46:45  iter: 939  total_loss: 14.13  loss_ce: 0.5683  loss_mask: 1.906  loss_dice: 3.103  loss_ce_0: 0.5499  loss_mask_0: 1.953  loss_dice_0: 3.286  loss_vq: 1.265  loss_vq_0: 1.265    time: 0.2968  last_time: 0.2899  data_time: 0.0049  last_data_time: 0.0036   lr: 9.859e-05  max_mem: 7621M
[12/02 00:37:41 d2.utils.events]:  eta: 4:46:37  iter: 959  total_loss: 14.19  loss_ce: 0.5282  loss_mask: 1.856  loss_dice: 3.21  loss_ce_0: 0.523  loss_mask_0: 1.856  loss_dice_0: 3.139  loss_vq: 1.308  loss_vq_0: 1.308    time: 0.2967  last_time: 0.2893  data_time: 0.0050  last_data_time: 0.0050   lr: 9.856e-05  max_mem: 7621M
[12/02 00:37:47 d2.utils.events]:  eta: 4:46:32  iter: 979  total_loss: 14.09  loss_ce: 0.6716  loss_mask: 1.729  loss_dice: 3.286  loss_ce_0: 0.6685  loss_mask_0: 1.721  loss_dice_0: 3.294  loss_vq: 1.244  loss_vq_0: 1.244    time: 0.2967  last_time: 0.2893  data_time: 0.0060  last_data_time: 0.0052   lr: 9.853e-05  max_mem: 7621M
[12/02 00:37:53 d2.utils.events]:  eta: 4:46:26  iter: 999  total_loss: 14.13  loss_ce: 0.6064  loss_mask: 1.74  loss_dice: 3.21  loss_ce_0: 0.6033  loss_mask_0: 1.697  loss_dice_0: 3.313  loss_vq: 1.291  loss_vq_0: 1.291    time: 0.2966  last_time: 0.3183  data_time: 0.0051  last_data_time: 0.0018   lr: 9.85e-05  max_mem: 7621M
[12/02 00:37:59 d2.utils.events]:  eta: 4:46:21  iter: 1019  total_loss: 14.14  loss_ce: 0.6844  loss_mask: 1.772  loss_dice: 3.492  loss_ce_0: 0.6869  loss_mask_0: 1.831  loss_dice_0: 3.458  loss_vq: 1.267  loss_vq_0: 1.267    time: 0.2966  last_time: 0.2915  data_time: 0.0050  last_data_time: 0.0043   lr: 9.847e-05  max_mem: 7621M
[12/02 00:38:05 d2.utils.events]:  eta: 4:46:13  iter: 1039  total_loss: 14.41  loss_ce: 0.7116  loss_mask: 1.93  loss_dice: 3.4  loss_ce_0: 0.711  loss_mask_0: 1.833  loss_dice_0: 3.421  loss_vq: 1.316  loss_vq_0: 1.316    time: 0.2966  last_time: 0.2847  data_time: 0.0054  last_data_time: 0.0031   lr: 9.844e-05  max_mem: 7621M
[12/02 00:38:11 d2.utils.events]:  eta: 4:46:07  iter: 1059  total_loss: 13.39  loss_ce: 0.5687  loss_mask: 1.773  loss_dice: 3.178  loss_ce_0: 0.5435  loss_mask_0: 1.839  loss_dice_0: 3.163  loss_vq: 1.316  loss_vq_0: 1.316    time: 0.2965  last_time: 0.2874  data_time: 0.0053  last_data_time: 0.0015   lr: 9.841e-05  max_mem: 7621M
[12/02 00:38:17 d2.utils.events]:  eta: 4:46:02  iter: 1079  total_loss: 14.27  loss_ce: 0.6156  loss_mask: 1.73  loss_dice: 3.263  loss_ce_0: 0.5973  loss_mask_0: 1.721  loss_dice_0: 3.235  loss_vq: 1.284  loss_vq_0: 1.284    time: 0.2964  last_time: 0.2901  data_time: 0.0054  last_data_time: 0.0033   lr: 9.838e-05  max_mem: 7621M
[12/02 00:38:23 d2.utils.events]:  eta: 4:45:59  iter: 1099  total_loss: 13.78  loss_ce: 0.5876  loss_mask: 1.758  loss_dice: 3.033  loss_ce_0: 0.5526  loss_mask_0: 1.745  loss_dice_0: 3.213  loss_vq: 1.337  loss_vq_0: 1.337    time: 0.2963  last_time: 0.2926  data_time: 0.0052  last_data_time: 0.0060   lr: 9.835e-05  max_mem: 7621M
[12/02 00:38:29 d2.utils.events]:  eta: 4:45:56  iter: 1119  total_loss: 14.23  loss_ce: 0.5712  loss_mask: 2.033  loss_dice: 3.329  loss_ce_0: 0.5236  loss_mask_0: 1.942  loss_dice_0: 3.34  loss_vq: 1.345  loss_vq_0: 1.345    time: 0.2965  last_time: 0.3215  data_time: 0.0058  last_data_time: 0.0032   lr: 9.832e-05  max_mem: 7621M
[12/02 00:38:35 d2.utils.events]:  eta: 4:45:47  iter: 1139  total_loss: 13.98  loss_ce: 0.6095  loss_mask: 1.744  loss_dice: 3.44  loss_ce_0: 0.5984  loss_mask_0: 1.73  loss_dice_0: 3.485  loss_vq: 1.292  loss_vq_0: 1.292    time: 0.2964  last_time: 0.2841  data_time: 0.0055  last_data_time: 0.0016   lr: 9.829e-05  max_mem: 7621M
[12/02 00:38:41 d2.utils.events]:  eta: 4:45:41  iter: 1159  total_loss: 14.12  loss_ce: 0.5395  loss_mask: 1.983  loss_dice: 3.108  loss_ce_0: 0.5375  loss_mask_0: 1.909  loss_dice_0: 3.147  loss_vq: 1.398  loss_vq_0: 1.398    time: 0.2963  last_time: 0.2966  data_time: 0.0053  last_data_time: 0.0078   lr: 9.826e-05  max_mem: 7621M
[12/02 00:38:47 d2.utils.events]:  eta: 4:45:36  iter: 1179  total_loss: 14.14  loss_ce: 0.6167  loss_mask: 1.816  loss_dice: 3.104  loss_ce_0: 0.5751  loss_mask_0: 1.901  loss_dice_0: 3.013  loss_vq: 1.34  loss_vq_0: 1.34    time: 0.2963  last_time: 0.2934  data_time: 0.0056  last_data_time: 0.0045   lr: 9.823e-05  max_mem: 7621M
[12/02 00:38:54 d2.utils.events]:  eta: 4:45:34  iter: 1199  total_loss: 14.31  loss_ce: 0.6813  loss_mask: 1.709  loss_dice: 3.446  loss_ce_0: 0.6459  loss_mask_0: 1.677  loss_dice_0: 3.406  loss_vq: 1.343  loss_vq_0: 1.343    time: 0.2965  last_time: 0.2894  data_time: 0.0056  last_data_time: 0.0047   lr: 9.82e-05  max_mem: 7621M
[12/02 00:39:00 d2.utils.events]:  eta: 4:45:31  iter: 1219  total_loss: 14.34  loss_ce: 0.6516  loss_mask: 1.87  loss_dice: 3.188  loss_ce_0: 0.6377  loss_mask_0: 1.789  loss_dice_0: 3.261  loss_vq: 1.291  loss_vq_0: 1.291    time: 0.2965  last_time: 0.2918  data_time: 0.0057  last_data_time: 0.0067   lr: 9.817e-05  max_mem: 7621M
[12/02 00:39:06 d2.utils.events]:  eta: 4:45:29  iter: 1239  total_loss: 13.65  loss_ce: 0.6151  loss_mask: 1.694  loss_dice: 3.269  loss_ce_0: 0.5951  loss_mask_0: 1.698  loss_dice_0: 3.301  loss_vq: 1.268  loss_vq_0: 1.268    time: 0.2965  last_time: 0.3155  data_time: 0.0050  last_data_time: 0.0093   lr: 9.814e-05  max_mem: 7621M
[12/02 00:39:12 d2.utils.events]:  eta: 4:45:23  iter: 1259  total_loss: 14.54  loss_ce: 0.6848  loss_mask: 1.775  loss_dice: 3.356  loss_ce_0: 0.6683  loss_mask_0: 1.823  loss_dice_0: 3.437  loss_vq: 1.349  loss_vq_0: 1.349    time: 0.2965  last_time: 0.2881  data_time: 0.0053  last_data_time: 0.0031   lr: 9.811e-05  max_mem: 7621M
[12/02 00:39:18 d2.utils.events]:  eta: 4:45:18  iter: 1279  total_loss: 13.99  loss_ce: 0.5205  loss_mask: 1.797  loss_dice: 3.076  loss_ce_0: 0.499  loss_mask_0: 1.797  loss_dice_0: 3.12  loss_vq: 1.341  loss_vq_0: 1.341    time: 0.2964  last_time: 0.2847  data_time: 0.0052  last_data_time: 0.0032   lr: 9.8079e-05  max_mem: 7621M
[12/02 00:39:24 d2.utils.events]:  eta: 4:45:11  iter: 1299  total_loss: 13.17  loss_ce: 0.5267  loss_mask: 1.571  loss_dice: 3.149  loss_ce_0: 0.5  loss_mask_0: 1.656  loss_dice_0: 3.176  loss_vq: 1.272  loss_vq_0: 1.272    time: 0.2963  last_time: 0.2912  data_time: 0.0050  last_data_time: 0.0045   lr: 9.8049e-05  max_mem: 7621M
[12/02 00:39:30 d2.utils.events]:  eta: 4:45:07  iter: 1319  total_loss: 14.02  loss_ce: 0.5517  loss_mask: 1.857  loss_dice: 3.241  loss_ce_0: 0.5377  loss_mask_0: 1.788  loss_dice_0: 3.134  loss_vq: 1.333  loss_vq_0: 1.333    time: 0.2962  last_time: 0.2946  data_time: 0.0051  last_data_time: 0.0054   lr: 9.8019e-05  max_mem: 7621M
[12/02 00:39:35 d2.utils.events]:  eta: 4:45:02  iter: 1339  total_loss: 14.46  loss_ce: 0.5761  loss_mask: 1.78  loss_dice: 3.197  loss_ce_0: 0.5749  loss_mask_0: 1.802  loss_dice_0: 3.347  loss_vq: 1.288  loss_vq_0: 1.288    time: 0.2962  last_time: 0.2916  data_time: 0.0053  last_data_time: 0.0051   lr: 9.7989e-05  max_mem: 7621M
[12/02 00:39:41 d2.utils.events]:  eta: 4:44:57  iter: 1359  total_loss: 13.85  loss_ce: 0.6288  loss_mask: 1.837  loss_dice: 3.275  loss_ce_0: 0.629  loss_mask_0: 1.793  loss_dice_0: 3.305  loss_vq: 1.358  loss_vq_0: 1.358    time: 0.2962  last_time: 0.2947  data_time: 0.0057  last_data_time: 0.0074   lr: 9.7959e-05  max_mem: 7621M
[12/02 00:39:47 d2.utils.events]:  eta: 4:44:53  iter: 1379  total_loss: 13.98  loss_ce: 0.5782  loss_mask: 1.66  loss_dice: 3.3  loss_ce_0: 0.5635  loss_mask_0: 1.675  loss_dice_0: 3.264  loss_vq: 1.258  loss_vq_0: 1.258    time: 0.2961  last_time: 0.2856  data_time: 0.0050  last_data_time: 0.0018   lr: 9.7929e-05  max_mem: 7621M
[12/02 00:39:53 d2.utils.events]:  eta: 4:44:46  iter: 1399  total_loss: 13.69  loss_ce: 0.6444  loss_mask: 1.71  loss_dice: 3.139  loss_ce_0: 0.6268  loss_mask_0: 1.705  loss_dice_0: 3.177  loss_vq: 1.239  loss_vq_0: 1.239    time: 0.2961  last_time: 0.2817  data_time: 0.0055  last_data_time: 0.0015   lr: 9.7899e-05  max_mem: 7621M
[12/02 00:39:59 d2.utils.events]:  eta: 4:44:41  iter: 1419  total_loss: 14.49  loss_ce: 0.7483  loss_mask: 1.775  loss_dice: 3.334  loss_ce_0: 0.7132  loss_mask_0: 1.8  loss_dice_0: 3.33  loss_vq: 1.344  loss_vq_0: 1.344    time: 0.2961  last_time: 0.2861  data_time: 0.0055  last_data_time: 0.0032   lr: 9.7869e-05  max_mem: 7621M
[12/02 00:40:05 d2.utils.events]:  eta: 4:44:40  iter: 1439  total_loss: 14.21  loss_ce: 0.7237  loss_mask: 1.792  loss_dice: 3.285  loss_ce_0: 0.6853  loss_mask_0: 1.761  loss_dice_0: 3.343  loss_vq: 1.319  loss_vq_0: 1.319    time: 0.2961  last_time: 0.2911  data_time: 0.0057  last_data_time: 0.0051   lr: 9.7839e-05  max_mem: 7621M
[12/02 00:40:12 d2.utils.events]:  eta: 4:44:34  iter: 1459  total_loss: 14.01  loss_ce: 0.5789  loss_mask: 1.778  loss_dice: 3.222  loss_ce_0: 0.5634  loss_mask_0: 1.785  loss_dice_0: 3.206  loss_vq: 1.428  loss_vq_0: 1.428    time: 0.2961  last_time: 0.3005  data_time: 0.0053  last_data_time: 0.0078   lr: 9.7809e-05  max_mem: 7621M
[12/02 00:40:18 d2.utils.events]:  eta: 4:44:26  iter: 1479  total_loss: 13.73  loss_ce: 0.5738  loss_mask: 1.771  loss_dice: 3.27  loss_ce_0: 0.5521  loss_mask_0: 1.728  loss_dice_0: 3.291  loss_vq: 1.351  loss_vq_0: 1.351    time: 0.2960  last_time: 0.2953  data_time: 0.0049  last_data_time: 0.0046   lr: 9.7779e-05  max_mem: 7621M
[12/02 00:40:24 d2.utils.events]:  eta: 4:44:20  iter: 1499  total_loss: 14.4  loss_ce: 0.5505  loss_mask: 1.949  loss_dice: 3.264  loss_ce_0: 0.5376  loss_mask_0: 2.044  loss_dice_0: 3.188  loss_vq: 1.38  loss_vq_0: 1.38    time: 0.2960  last_time: 0.3227  data_time: 0.0052  last_data_time: 0.0022   lr: 9.7749e-05  max_mem: 7621M
[12/02 00:40:30 d2.utils.events]:  eta: 4:44:15  iter: 1519  total_loss: 13.73  loss_ce: 0.6315  loss_mask: 1.59  loss_dice: 3.092  loss_ce_0: 0.6116  loss_mask_0: 1.681  loss_dice_0: 3.165  loss_vq: 1.396  loss_vq_0: 1.396    time: 0.2961  last_time: 0.2964  data_time: 0.0053  last_data_time: 0.0063   lr: 9.7719e-05  max_mem: 7621M
[12/02 00:40:36 d2.utils.events]:  eta: 4:44:11  iter: 1539  total_loss: 13.79  loss_ce: 0.5486  loss_mask: 1.867  loss_dice: 3.097  loss_ce_0: 0.5516  loss_mask_0: 1.773  loss_dice_0: 3.262  loss_vq: 1.363  loss_vq_0: 1.363    time: 0.2960  last_time: 0.2860  data_time: 0.0048  last_data_time: 0.0019   lr: 9.7689e-05  max_mem: 7621M
[12/02 00:40:42 d2.utils.events]:  eta: 4:44:06  iter: 1559  total_loss: 14.02  loss_ce: 0.5626  loss_mask: 1.822  loss_dice: 3.192  loss_ce_0: 0.5432  loss_mask_0: 1.857  loss_dice_0: 3.264  loss_vq: 1.39  loss_vq_0: 1.39    time: 0.2960  last_time: 0.2903  data_time: 0.0051  last_data_time: 0.0056   lr: 9.7658e-05  max_mem: 7621M
[12/02 00:40:48 d2.utils.events]:  eta: 4:44:03  iter: 1579  total_loss: 13.56  loss_ce: 0.5254  loss_mask: 1.717  loss_dice: 3.202  loss_ce_0: 0.5381  loss_mask_0: 1.76  loss_dice_0: 3.199  loss_vq: 1.332  loss_vq_0: 1.332    time: 0.2960  last_time: 0.2940  data_time: 0.0057  last_data_time: 0.0050   lr: 9.7628e-05  max_mem: 7621M
[12/02 00:40:54 d2.utils.events]:  eta: 4:43:58  iter: 1599  total_loss: 14.22  loss_ce: 0.7082  loss_mask: 1.848  loss_dice: 3.284  loss_ce_0: 0.6708  loss_mask_0: 1.829  loss_dice_0: 3.342  loss_vq: 1.339  loss_vq_0: 1.339    time: 0.2959  last_time: 0.2906  data_time: 0.0052  last_data_time: 0.0051   lr: 9.7598e-05  max_mem: 7621M
[12/02 00:41:00 d2.utils.events]:  eta: 4:43:56  iter: 1619  total_loss: 14.07  loss_ce: 0.545  loss_mask: 1.821  loss_dice: 3.175  loss_ce_0: 0.5436  loss_mask_0: 1.702  loss_dice_0: 3.23  loss_vq: 1.381  loss_vq_0: 1.381    time: 0.2959  last_time: 0.2880  data_time: 0.0055  last_data_time: 0.0033   lr: 9.7568e-05  max_mem: 7621M
[12/02 00:41:06 d2.utils.events]:  eta: 4:43:50  iter: 1639  total_loss: 13.31  loss_ce: 0.4923  loss_mask: 1.909  loss_dice: 3.03  loss_ce_0: 0.4853  loss_mask_0: 1.87  loss_dice_0: 3.034  loss_vq: 1.374  loss_vq_0: 1.374    time: 0.2959  last_time: 0.3236  data_time: 0.0051  last_data_time: 0.0053   lr: 9.7538e-05  max_mem: 7621M
[12/02 00:41:12 d2.utils.events]:  eta: 4:43:45  iter: 1659  total_loss: 13.74  loss_ce: 0.5289  loss_mask: 1.863  loss_dice: 3.155  loss_ce_0: 0.5039  loss_mask_0: 1.782  loss_dice_0: 3.179  loss_vq: 1.465  loss_vq_0: 1.465    time: 0.2959  last_time: 0.2842  data_time: 0.0049  last_data_time: 0.0017   lr: 9.7508e-05  max_mem: 7621M
[12/02 00:41:18 d2.utils.events]:  eta: 4:43:40  iter: 1679  total_loss: 13.44  loss_ce: 0.6005  loss_mask: 1.503  loss_dice: 3.135  loss_ce_0: 0.581  loss_mask_0: 1.554  loss_dice_0: 3.204  loss_vq: 1.338  loss_vq_0: 1.338    time: 0.2959  last_time: 0.2944  data_time: 0.0055  last_data_time: 0.0046   lr: 9.7478e-05  max_mem: 7621M
[12/02 00:41:24 d2.utils.events]:  eta: 4:43:35  iter: 1699  total_loss: 14.41  loss_ce: 0.7385  loss_mask: 1.615  loss_dice: 3.396  loss_ce_0: 0.7269  loss_mask_0: 1.669  loss_dice_0: 3.341  loss_vq: 1.401  loss_vq_0: 1.401    time: 0.2959  last_time: 0.2875  data_time: 0.0055  last_data_time: 0.0032   lr: 9.7448e-05  max_mem: 7621M
[12/02 00:41:30 d2.utils.events]:  eta: 4:43:36  iter: 1719  total_loss: 14.13  loss_ce: 0.587  loss_mask: 1.809  loss_dice: 3.203  loss_ce_0: 0.5831  loss_mask_0: 1.71  loss_dice_0: 3.223  loss_vq: 1.416  loss_vq_0: 1.416    time: 0.2960  last_time: 0.2980  data_time: 0.0059  last_data_time: 0.0104   lr: 9.7418e-05  max_mem: 7621M
[12/02 00:41:36 d2.utils.events]:  eta: 4:43:32  iter: 1739  total_loss: 13.99  loss_ce: 0.567  loss_mask: 1.709  loss_dice: 3.29  loss_ce_0: 0.5634  loss_mask_0: 1.751  loss_dice_0: 3.218  loss_vq: 1.391  loss_vq_0: 1.391    time: 0.2961  last_time: 0.2932  data_time: 0.0061  last_data_time: 0.0046   lr: 9.7388e-05  max_mem: 7621M
[12/02 00:41:42 d2.utils.events]:  eta: 4:43:24  iter: 1759  total_loss: 14.16  loss_ce: 0.5243  loss_mask: 1.832  loss_dice: 3.344  loss_ce_0: 0.5457  loss_mask_0: 1.703  loss_dice_0: 3.375  loss_vq: 1.353  loss_vq_0: 1.353    time: 0.2960  last_time: 0.2920  data_time: 0.0053  last_data_time: 0.0043   lr: 9.7358e-05  max_mem: 7621M
[12/02 00:41:48 d2.utils.events]:  eta: 4:43:18  iter: 1779  total_loss: 13.55  loss_ce: 0.4995  loss_mask: 1.593  loss_dice: 3.113  loss_ce_0: 0.4862  loss_mask_0: 1.636  loss_dice_0: 3.118  loss_vq: 1.379  loss_vq_0: 1.379    time: 0.2960  last_time: 0.2844  data_time: 0.0053  last_data_time: 0.0017   lr: 9.7328e-05  max_mem: 7621M
[12/02 00:41:54 d2.utils.events]:  eta: 4:43:15  iter: 1799  total_loss: 13.53  loss_ce: 0.4995  loss_mask: 1.705  loss_dice: 3.077  loss_ce_0: 0.504  loss_mask_0: 1.741  loss_dice_0: 3.055  loss_vq: 1.449  loss_vq_0: 1.449    time: 0.2961  last_time: 0.2934  data_time: 0.0053  last_data_time: 0.0057   lr: 9.7297e-05  max_mem: 7621M
[12/02 00:42:00 d2.utils.events]:  eta: 4:43:10  iter: 1819  total_loss: 12.94  loss_ce: 0.5013  loss_mask: 1.803  loss_dice: 2.669  loss_ce_0: 0.4628  loss_mask_0: 1.74  loss_dice_0: 2.645  loss_vq: 1.424  loss_vq_0: 1.424    time: 0.2961  last_time: 0.2918  data_time: 0.0048  last_data_time: 0.0047   lr: 9.7267e-05  max_mem: 7621M
[12/02 00:42:06 d2.utils.events]:  eta: 4:43:08  iter: 1839  total_loss: 13.55  loss_ce: 0.6062  loss_mask: 1.638  loss_dice: 3.034  loss_ce_0: 0.5786  loss_mask_0: 1.727  loss_dice_0: 3.084  loss_vq: 1.438  loss_vq_0: 1.438    time: 0.2961  last_time: 0.2920  data_time: 0.0057  last_data_time: 0.0071   lr: 9.7237e-05  max_mem: 7621M
[12/02 00:42:12 d2.utils.events]:  eta: 4:43:04  iter: 1859  total_loss: 13.5  loss_ce: 0.422  loss_mask: 1.946  loss_dice: 2.803  loss_ce_0: 0.4286  loss_mask_0: 1.951  loss_dice_0: 2.85  loss_vq: 1.467  loss_vq_0: 1.467    time: 0.2961  last_time: 0.2884  data_time: 0.0057  last_data_time: 0.0047   lr: 9.7207e-05  max_mem: 7621M
[12/02 00:42:18 d2.utils.events]:  eta: 4:42:55  iter: 1879  total_loss: 14.23  loss_ce: 0.5709  loss_mask: 1.871  loss_dice: 3.223  loss_ce_0: 0.5596  loss_mask_0: 1.936  loss_dice_0: 3.157  loss_vq: 1.429  loss_vq_0: 1.429    time: 0.2961  last_time: 0.2894  data_time: 0.0053  last_data_time: 0.0048   lr: 9.7177e-05  max_mem: 7621M
[12/02 00:42:25 d2.utils.events]:  eta: 4:42:48  iter: 1899  total_loss: 14.03  loss_ce: 0.5543  loss_mask: 1.791  loss_dice: 3.027  loss_ce_0: 0.5348  loss_mask_0: 1.804  loss_dice_0: 3.075  loss_vq: 1.524  loss_vq_0: 1.524    time: 0.2961  last_time: 0.2930  data_time: 0.0050  last_data_time: 0.0046   lr: 9.7147e-05  max_mem: 7621M
[12/02 00:42:30 d2.utils.events]:  eta: 4:42:42  iter: 1919  total_loss: 14.3  loss_ce: 0.5385  loss_mask: 1.721  loss_dice: 3.198  loss_ce_0: 0.5724  loss_mask_0: 1.848  loss_dice_0: 3.214  loss_vq: 1.377  loss_vq_0: 1.377    time: 0.2961  last_time: 0.2924  data_time: 0.0052  last_data_time: 0.0064   lr: 9.7117e-05  max_mem: 7621M
[12/02 00:42:37 d2.utils.events]:  eta: 4:42:43  iter: 1939  total_loss: 13.56  loss_ce: 0.5388  loss_mask: 1.759  loss_dice: 3.108  loss_ce_0: 0.5234  loss_mask_0: 1.696  loss_dice_0: 3.115  loss_vq: 1.458  loss_vq_0: 1.458    time: 0.2961  last_time: 0.2953  data_time: 0.0060  last_data_time: 0.0065   lr: 9.7087e-05  max_mem: 7621M
[12/02 00:42:42 d2.utils.events]:  eta: 4:42:39  iter: 1959  total_loss: 13.99  loss_ce: 0.6641  loss_mask: 1.641  loss_dice: 3.319  loss_ce_0: 0.6406  loss_mask_0: 1.675  loss_dice_0: 3.493  loss_vq: 1.378  loss_vq_0: 1.378    time: 0.2961  last_time: 0.2891  data_time: 0.0053  last_data_time: 0.0050   lr: 9.7057e-05  max_mem: 7621M
[12/02 00:42:49 d2.utils.events]:  eta: 4:42:35  iter: 1979  total_loss: 13.46  loss_ce: 0.6466  loss_mask: 1.614  loss_dice: 3.166  loss_ce_0: 0.6226  loss_mask_0: 1.662  loss_dice_0: 3.18  loss_vq: 1.451  loss_vq_0: 1.451    time: 0.2962  last_time: 0.3241  data_time: 0.0057  last_data_time: 0.0034   lr: 9.7027e-05  max_mem: 7621M
[12/02 00:42:55 d2.utils.events]:  eta: 4:42:34  iter: 1999  total_loss: 13.89  loss_ce: 0.5452  loss_mask: 1.804  loss_dice: 2.963  loss_ce_0: 0.5307  loss_mask_0: 1.843  loss_dice_0: 2.916  loss_vq: 1.595  loss_vq_0: 1.595    time: 0.2962  last_time: 0.2965  data_time: 0.0052  last_data_time: 0.0049   lr: 9.6996e-05  max_mem: 7621M
[12/02 00:43:01 d2.utils.events]:  eta: 4:42:29  iter: 2019  total_loss: 13.73  loss_ce: 0.456  loss_mask: 1.734  loss_dice: 3.02  loss_ce_0: 0.4653  loss_mask_0: 1.794  loss_dice_0: 3.044  loss_vq: 1.512  loss_vq_0: 1.512    time: 0.2963  last_time: 0.2911  data_time: 0.0052  last_data_time: 0.0046   lr: 9.6966e-05  max_mem: 7621M
[12/02 00:43:07 d2.utils.events]:  eta: 4:42:26  iter: 2039  total_loss: 13.82  loss_ce: 0.6166  loss_mask: 1.596  loss_dice: 3.249  loss_ce_0: 0.6321  loss_mask_0: 1.702  loss_dice_0: 3.256  loss_vq: 1.399  loss_vq_0: 1.399    time: 0.2963  last_time: 0.3081  data_time: 0.0053  last_data_time: 0.0066   lr: 9.6936e-05  max_mem: 7621M
[12/02 00:43:13 d2.utils.events]:  eta: 4:42:24  iter: 2059  total_loss: 14.49  loss_ce: 0.5406  loss_mask: 1.999  loss_dice: 3.041  loss_ce_0: 0.5116  loss_mask_0: 2.012  loss_dice_0: 3.049  loss_vq: 1.588  loss_vq_0: 1.588    time: 0.2963  last_time: 0.2914  data_time: 0.0051  last_data_time: 0.0050   lr: 9.6906e-05  max_mem: 7621M
[12/02 00:43:19 d2.utils.events]:  eta: 4:42:14  iter: 2079  total_loss: 14.55  loss_ce: 0.6104  loss_mask: 1.69  loss_dice: 3.4  loss_ce_0: 0.5722  loss_mask_0: 1.715  loss_dice_0: 3.362  loss_vq: 1.583  loss_vq_0: 1.583    time: 0.2962  last_time: 0.2893  data_time: 0.0052  last_data_time: 0.0054   lr: 9.6876e-05  max_mem: 7621M
[12/02 00:43:25 d2.utils.events]:  eta: 4:42:13  iter: 2099  total_loss: 14.01  loss_ce: 0.6001  loss_mask: 1.714  loss_dice: 3.145  loss_ce_0: 0.56  loss_mask_0: 1.691  loss_dice_0: 3.149  loss_vq: 1.464  loss_vq_0: 1.464    time: 0.2962  last_time: 0.2928  data_time: 0.0058  last_data_time: 0.0050   lr: 9.6846e-05  max_mem: 7621M
[12/02 00:43:31 d2.utils.events]:  eta: 4:42:05  iter: 2119  total_loss: 14.22  loss_ce: 0.568  loss_mask: 1.752  loss_dice: 3.142  loss_ce_0: 0.5568  loss_mask_0: 1.708  loss_dice_0: 3.155  loss_vq: 1.553  loss_vq_0: 1.553    time: 0.2962  last_time: 0.2959  data_time: 0.0058  last_data_time: 0.0057   lr: 9.6816e-05  max_mem: 7621M
[12/02 00:43:37 d2.utils.events]:  eta: 4:42:04  iter: 2139  total_loss: 14.01  loss_ce: 0.5677  loss_mask: 1.633  loss_dice: 3.068  loss_ce_0: 0.5208  loss_mask_0: 1.65  loss_dice_0: 3.065  loss_vq: 1.514  loss_vq_0: 1.514    time: 0.2962  last_time: 0.3022  data_time: 0.0054  last_data_time: 0.0050   lr: 9.6786e-05  max_mem: 7621M
[12/02 00:43:43 d2.utils.events]:  eta: 4:42:00  iter: 2159  total_loss: 14.3  loss_ce: 0.5615  loss_mask: 1.938  loss_dice: 2.974  loss_ce_0: 0.56  loss_mask_0: 1.893  loss_dice_0: 3.087  loss_vq: 1.615  loss_vq_0: 1.615    time: 0.2962  last_time: 0.2939  data_time: 0.0056  last_data_time: 0.0046   lr: 9.6756e-05  max_mem: 7621M
[12/02 00:43:49 d2.utils.events]:  eta: 4:41:52  iter: 2179  total_loss: 14.56  loss_ce: 0.6601  loss_mask: 1.77  loss_dice: 3.275  loss_ce_0: 0.6567  loss_mask_0: 1.812  loss_dice_0: 3.277  loss_vq: 1.596  loss_vq_0: 1.596    time: 0.2962  last_time: 0.3298  data_time: 0.0056  last_data_time: 0.0063   lr: 9.6725e-05  max_mem: 7621M
[12/02 00:43:55 d2.utils.events]:  eta: 4:41:46  iter: 2199  total_loss: 14.22  loss_ce: 0.691  loss_mask: 1.714  loss_dice: 3.044  loss_ce_0: 0.6836  loss_mask_0: 1.682  loss_dice_0: 3.089  loss_vq: 1.487  loss_vq_0: 1.487    time: 0.2963  last_time: 0.2885  data_time: 0.0063  last_data_time: 0.0038   lr: 9.6695e-05  max_mem: 7621M
[12/02 00:44:01 d2.utils.events]:  eta: 4:41:37  iter: 2219  total_loss: 13.57  loss_ce: 0.6266  loss_mask: 1.652  loss_dice: 2.889  loss_ce_0: 0.6149  loss_mask_0: 1.699  loss_dice_0: 3.046  loss_vq: 1.517  loss_vq_0: 1.517    time: 0.2962  last_time: 0.2903  data_time: 0.0053  last_data_time: 0.0049   lr: 9.6665e-05  max_mem: 7621M
[12/02 00:44:07 d2.utils.events]:  eta: 4:41:28  iter: 2239  total_loss: 13.92  loss_ce: 0.5688  loss_mask: 1.808  loss_dice: 3.019  loss_ce_0: 0.5319  loss_mask_0: 1.795  loss_dice_0: 3.005  loss_vq: 1.511  loss_vq_0: 1.511    time: 0.2962  last_time: 0.2863  data_time: 0.0050  last_data_time: 0.0016   lr: 9.6635e-05  max_mem: 7621M
[12/02 00:44:13 d2.utils.events]:  eta: 4:41:25  iter: 2259  total_loss: 14  loss_ce: 0.5442  loss_mask: 1.834  loss_dice: 3.019  loss_ce_0: 0.5187  loss_mask_0: 1.899  loss_dice_0: 3.08  loss_vq: 1.538  loss_vq_0: 1.538    time: 0.2962  last_time: 0.2886  data_time: 0.0053  last_data_time: 0.0050   lr: 9.6605e-05  max_mem: 7621M
[12/02 00:44:19 d2.utils.events]:  eta: 4:41:23  iter: 2279  total_loss: 13.92  loss_ce: 0.5495  loss_mask: 1.607  loss_dice: 3.251  loss_ce_0: 0.5495  loss_mask_0: 1.603  loss_dice_0: 3.246  loss_vq: 1.527  loss_vq_0: 1.527    time: 0.2962  last_time: 0.2936  data_time: 0.0054  last_data_time: 0.0061   lr: 9.6575e-05  max_mem: 7621M
[12/02 00:44:25 d2.utils.events]:  eta: 4:41:17  iter: 2299  total_loss: 14.49  loss_ce: 0.594  loss_mask: 1.623  loss_dice: 3.311  loss_ce_0: 0.581  loss_mask_0: 1.636  loss_dice_0: 3.342  loss_vq: 1.511  loss_vq_0: 1.511    time: 0.2961  last_time: 0.3324  data_time: 0.0052  last_data_time: 0.0102   lr: 9.6545e-05  max_mem: 7621M
[12/02 00:44:31 d2.utils.events]:  eta: 4:41:13  iter: 2319  total_loss: 14.06  loss_ce: 0.6013  loss_mask: 1.807  loss_dice: 3.12  loss_ce_0: 0.611  loss_mask_0: 1.767  loss_dice_0: 3.03  loss_vq: 1.712  loss_vq_0: 1.712    time: 0.2963  last_time: 0.2889  data_time: 0.0067  last_data_time: 0.0046   lr: 9.6515e-05  max_mem: 7621M
[12/02 00:44:38 d2.utils.events]:  eta: 4:41:10  iter: 2339  total_loss: 13.65  loss_ce: 0.5598  loss_mask: 1.743  loss_dice: 3.148  loss_ce_0: 0.5453  loss_mask_0: 1.761  loss_dice_0: 3.247  loss_vq: 1.572  loss_vq_0: 1.572    time: 0.2963  last_time: 0.2902  data_time: 0.0057  last_data_time: 0.0032   lr: 9.6485e-05  max_mem: 7621M
[12/02 00:44:44 d2.utils.events]:  eta: 4:41:08  iter: 2359  total_loss: 13.98  loss_ce: 0.5798  loss_mask: 1.872  loss_dice: 3.142  loss_ce_0: 0.5653  loss_mask_0: 1.868  loss_dice_0: 3.011  loss_vq: 1.587  loss_vq_0: 1.587    time: 0.2963  last_time: 0.3033  data_time: 0.0051  last_data_time: 0.0049   lr: 9.6454e-05  max_mem: 7621M
[12/02 00:44:50 d2.utils.events]:  eta: 4:41:03  iter: 2379  total_loss: 14.39  loss_ce: 0.6561  loss_mask: 1.612  loss_dice: 3.11  loss_ce_0: 0.6164  loss_mask_0: 1.61  loss_dice_0: 3.176  loss_vq: 1.649  loss_vq_0: 1.649    time: 0.2963  last_time: 0.2949  data_time: 0.0051  last_data_time: 0.0061   lr: 9.6424e-05  max_mem: 7621M
[12/02 00:44:56 d2.utils.events]:  eta: 4:40:57  iter: 2399  total_loss: 13.39  loss_ce: 0.4249  loss_mask: 1.735  loss_dice: 2.792  loss_ce_0: 0.4172  loss_mask_0: 1.845  loss_dice_0: 2.966  loss_vq: 1.773  loss_vq_0: 1.773    time: 0.2963  last_time: 0.2885  data_time: 0.0049  last_data_time: 0.0032   lr: 9.6394e-05  max_mem: 7621M
[12/02 00:45:02 d2.utils.events]:  eta: 4:40:51  iter: 2419  total_loss: 14.31  loss_ce: 0.5061  loss_mask: 1.934  loss_dice: 3.024  loss_ce_0: 0.4919  loss_mask_0: 1.901  loss_dice_0: 2.951  loss_vq: 1.844  loss_vq_0: 1.844    time: 0.2963  last_time: 0.2893  data_time: 0.0057  last_data_time: 0.0045   lr: 9.6364e-05  max_mem: 7621M
[12/02 00:45:08 d2.utils.events]:  eta: 4:40:42  iter: 2439  total_loss: 14.01  loss_ce: 0.4287  loss_mask: 1.982  loss_dice: 2.839  loss_ce_0: 0.421  loss_mask_0: 1.902  loss_dice_0: 3.019  loss_vq: 1.662  loss_vq_0: 1.662    time: 0.2963  last_time: 0.2947  data_time: 0.0056  last_data_time: 0.0082   lr: 9.6334e-05  max_mem: 7621M
[12/02 00:45:14 d2.utils.events]:  eta: 4:40:35  iter: 2459  total_loss: 14.26  loss_ce: 0.5253  loss_mask: 1.456  loss_dice: 2.913  loss_ce_0: 0.5163  loss_mask_0: 1.388  loss_dice_0: 2.942  loss_vq: 1.856  loss_vq_0: 1.856    time: 0.2963  last_time: 0.2982  data_time: 0.0055  last_data_time: 0.0086   lr: 9.6304e-05  max_mem: 7621M
[12/02 00:45:20 d2.utils.events]:  eta: 4:40:30  iter: 2479  total_loss: 13.99  loss_ce: 0.5298  loss_mask: 1.712  loss_dice: 2.976  loss_ce_0: 0.5333  loss_mask_0: 1.679  loss_dice_0: 3.094  loss_vq: 1.743  loss_vq_0: 1.743    time: 0.2963  last_time: 0.2912  data_time: 0.0055  last_data_time: 0.0067   lr: 9.6274e-05  max_mem: 7621M
[12/02 00:45:26 d2.utils.events]:  eta: 4:40:26  iter: 2499  total_loss: 14.77  loss_ce: 0.4357  loss_mask: 2.013  loss_dice: 3.093  loss_ce_0: 0.4317  loss_mask_0: 1.973  loss_dice_0: 3.133  loss_vq: 1.768  loss_vq_0: 1.768    time: 0.2962  last_time: 0.2889  data_time: 0.0052  last_data_time: 0.0037   lr: 9.6244e-05  max_mem: 7621M
[12/02 00:45:32 d2.utils.events]:  eta: 4:40:17  iter: 2519  total_loss: 13.93  loss_ce: 0.4542  loss_mask: 1.805  loss_dice: 2.924  loss_ce_0: 0.4229  loss_mask_0: 1.849  loss_dice_0: 2.972  loss_vq: 1.814  loss_vq_0: 1.814    time: 0.2962  last_time: 0.2984  data_time: 0.0050  last_data_time: 0.0085   lr: 9.6213e-05  max_mem: 7621M
[12/02 00:45:38 d2.utils.events]:  eta: 4:40:12  iter: 2539  total_loss: 15.04  loss_ce: 0.5307  loss_mask: 1.807  loss_dice: 3.141  loss_ce_0: 0.51  loss_mask_0: 1.783  loss_dice_0: 3.123  loss_vq: 1.885  loss_vq_0: 1.885    time: 0.2962  last_time: 0.2842  data_time: 0.0051  last_data_time: 0.0018   lr: 9.6183e-05  max_mem: 7621M
[12/02 00:45:44 d2.utils.events]:  eta: 4:40:08  iter: 2559  total_loss: 15.09  loss_ce: 0.5781  loss_mask: 1.835  loss_dice: 3.179  loss_ce_0: 0.5691  loss_mask_0: 1.834  loss_dice_0: 3.145  loss_vq: 1.854  loss_vq_0: 1.854    time: 0.2962  last_time: 0.2852  data_time: 0.0054  last_data_time: 0.0035   lr: 9.6153e-05  max_mem: 7621M
[12/02 00:45:50 d2.utils.events]:  eta: 4:39:58  iter: 2579  total_loss: 14.76  loss_ce: 0.6072  loss_mask: 1.664  loss_dice: 3.089  loss_ce_0: 0.6081  loss_mask_0: 1.72  loss_dice_0: 3.184  loss_vq: 1.874  loss_vq_0: 1.874    time: 0.2961  last_time: 0.2894  data_time: 0.0055  last_data_time: 0.0046   lr: 9.6123e-05  max_mem: 7621M
[12/02 00:45:56 d2.utils.events]:  eta: 4:39:51  iter: 2599  total_loss: 14.33  loss_ce: 0.5145  loss_mask: 1.928  loss_dice: 2.893  loss_ce_0: 0.4986  loss_mask_0: 1.914  loss_dice_0: 2.995  loss_vq: 1.881  loss_vq_0: 1.881    time: 0.2961  last_time: 0.2972  data_time: 0.0057  last_data_time: 0.0070   lr: 9.6093e-05  max_mem: 7621M
[12/02 00:46:02 d2.utils.events]:  eta: 4:39:45  iter: 2619  total_loss: 14.68  loss_ce: 0.5154  loss_mask: 1.608  loss_dice: 3.002  loss_ce_0: 0.5175  loss_mask_0: 1.618  loss_dice_0: 2.932  loss_vq: 1.892  loss_vq_0: 1.892    time: 0.2961  last_time: 0.3208  data_time: 0.0051  last_data_time: 0.0063   lr: 9.6063e-05  max_mem: 7622M
[12/02 00:46:08 d2.utils.events]:  eta: 4:39:41  iter: 2639  total_loss: 13.91  loss_ce: 0.4706  loss_mask: 1.725  loss_dice: 2.863  loss_ce_0: 0.4727  loss_mask_0: 1.743  loss_dice_0: 2.834  loss_vq: 1.934  loss_vq_0: 1.934    time: 0.2962  last_time: 0.3205  data_time: 0.0056  last_data_time: 0.0051   lr: 9.6033e-05  max_mem: 7622M
[12/02 00:46:14 d2.utils.events]:  eta: 4:39:33  iter: 2659  total_loss: 14.49  loss_ce: 0.6856  loss_mask: 1.71  loss_dice: 2.971  loss_ce_0: 0.6499  loss_mask_0: 1.785  loss_dice_0: 3.006  loss_vq: 1.868  loss_vq_0: 1.868    time: 0.2961  last_time: 0.2908  data_time: 0.0053  last_data_time: 0.0057   lr: 9.6003e-05  max_mem: 7622M
[12/02 00:46:20 d2.utils.events]:  eta: 4:39:26  iter: 2679  total_loss: 14.83  loss_ce: 0.6051  loss_mask: 1.684  loss_dice: 2.881  loss_ce_0: 0.5544  loss_mask_0: 1.667  loss_dice_0: 2.992  loss_vq: 1.859  loss_vq_0: 1.859    time: 0.2961  last_time: 0.2900  data_time: 0.0051  last_data_time: 0.0045   lr: 9.5972e-05  max_mem: 7622M
[12/02 00:46:26 d2.utils.events]:  eta: 4:39:21  iter: 2699  total_loss: 14.99  loss_ce: 0.5741  loss_mask: 1.625  loss_dice: 3.155  loss_ce_0: 0.5617  loss_mask_0: 1.608  loss_dice_0: 3.253  loss_vq: 1.918  loss_vq_0: 1.918    time: 0.2961  last_time: 0.2879  data_time: 0.0055  last_data_time: 0.0034   lr: 9.5942e-05  max_mem: 7622M
[12/02 00:46:32 d2.utils.events]:  eta: 4:39:12  iter: 2719  total_loss: 15.2  loss_ce: 0.4431  loss_mask: 1.837  loss_dice: 2.998  loss_ce_0: 0.4405  loss_mask_0: 1.849  loss_dice_0: 3.062  loss_vq: 1.894  loss_vq_0: 1.894    time: 0.2961  last_time: 0.2899  data_time: 0.0050  last_data_time: 0.0049   lr: 9.5912e-05  max_mem: 7622M
[12/02 00:46:38 d2.utils.events]:  eta: 4:38:57  iter: 2739  total_loss: 14.54  loss_ce: 0.5209  loss_mask: 1.796  loss_dice: 3.108  loss_ce_0: 0.5235  loss_mask_0: 1.828  loss_dice_0: 3.073  loss_vq: 1.804  loss_vq_0: 1.804    time: 0.2961  last_time: 0.2896  data_time: 0.0052  last_data_time: 0.0050   lr: 9.5882e-05  max_mem: 7622M
[12/02 00:46:44 d2.utils.events]:  eta: 4:38:49  iter: 2759  total_loss: 14.46  loss_ce: 0.52  loss_mask: 1.634  loss_dice: 2.892  loss_ce_0: 0.5153  loss_mask_0: 1.597  loss_dice_0: 2.957  loss_vq: 2.012  loss_vq_0: 2.012    time: 0.2961  last_time: 0.2950  data_time: 0.0051  last_data_time: 0.0065   lr: 9.5852e-05  max_mem: 7622M
[12/02 00:46:50 d2.utils.events]:  eta: 4:38:38  iter: 2779  total_loss: 14.12  loss_ce: 0.5224  loss_mask: 1.629  loss_dice: 2.962  loss_ce_0: 0.5052  loss_mask_0: 1.703  loss_dice_0: 2.966  loss_vq: 1.877  loss_vq_0: 1.877    time: 0.2960  last_time: 0.2879  data_time: 0.0050  last_data_time: 0.0041   lr: 9.5822e-05  max_mem: 7622M
[12/02 00:46:56 d2.utils.events]:  eta: 4:38:25  iter: 2799  total_loss: 15.25  loss_ce: 0.5194  loss_mask: 1.77  loss_dice: 2.958  loss_ce_0: 0.5151  loss_mask_0: 1.821  loss_dice_0: 3.02  loss_vq: 2.12  loss_vq_0: 2.12    time: 0.2960  last_time: 0.2922  data_time: 0.0052  last_data_time: 0.0049   lr: 9.5792e-05  max_mem: 7622M
[12/02 00:47:02 d2.utils.events]:  eta: 4:38:21  iter: 2819  total_loss: 15.28  loss_ce: 0.5967  loss_mask: 1.929  loss_dice: 3.24  loss_ce_0: 0.5865  loss_mask_0: 1.939  loss_dice_0: 3.258  loss_vq: 2.054  loss_vq_0: 2.054    time: 0.2960  last_time: 0.2936  data_time: 0.0055  last_data_time: 0.0079   lr: 9.5761e-05  max_mem: 7622M
[12/02 00:47:08 d2.utils.events]:  eta: 4:38:18  iter: 2839  total_loss: 14.52  loss_ce: 0.6019  loss_mask: 1.846  loss_dice: 2.945  loss_ce_0: 0.6074  loss_mask_0: 1.818  loss_dice_0: 3.052  loss_vq: 1.918  loss_vq_0: 1.918    time: 0.2960  last_time: 0.2926  data_time: 0.0053  last_data_time: 0.0055   lr: 9.5731e-05  max_mem: 7622M
[12/02 00:47:14 d2.utils.events]:  eta: 4:38:05  iter: 2859  total_loss: 14.84  loss_ce: 0.5256  loss_mask: 1.807  loss_dice: 3.079  loss_ce_0: 0.536  loss_mask_0: 1.815  loss_dice_0: 3.126  loss_vq: 1.995  loss_vq_0: 1.995    time: 0.2959  last_time: 0.2977  data_time: 0.0054  last_data_time: 0.0117   lr: 9.5701e-05  max_mem: 7622M
[12/02 00:47:20 d2.utils.events]:  eta: 4:37:59  iter: 2879  total_loss: 15.13  loss_ce: 0.5221  loss_mask: 1.837  loss_dice: 3.156  loss_ce_0: 0.5106  loss_mask_0: 1.788  loss_dice_0: 3.178  loss_vq: 2.175  loss_vq_0: 2.175    time: 0.2959  last_time: 0.2889  data_time: 0.0048  last_data_time: 0.0050   lr: 9.5671e-05  max_mem: 7622M
[12/02 00:47:26 d2.utils.events]:  eta: 4:37:52  iter: 2899  total_loss: 14.53  loss_ce: 0.577  loss_mask: 1.655  loss_dice: 2.988  loss_ce_0: 0.5634  loss_mask_0: 1.655  loss_dice_0: 3.057  loss_vq: 1.763  loss_vq_0: 1.763    time: 0.2959  last_time: 0.2906  data_time: 0.0050  last_data_time: 0.0045   lr: 9.5641e-05  max_mem: 7622M
[12/02 00:47:32 d2.utils.events]:  eta: 4:37:40  iter: 2919  total_loss: 15  loss_ce: 0.699  loss_mask: 1.768  loss_dice: 3.159  loss_ce_0: 0.6719  loss_mask_0: 1.733  loss_dice_0: 3.22  loss_vq: 1.98  loss_vq_0: 1.98    time: 0.2958  last_time: 0.3176  data_time: 0.0051  last_data_time: 0.0123   lr: 9.5611e-05  max_mem: 7622M
[12/02 00:47:38 d2.utils.events]:  eta: 4:37:32  iter: 2939  total_loss: 14.21  loss_ce: 0.5326  loss_mask: 1.504  loss_dice: 3.109  loss_ce_0: 0.5296  loss_mask_0: 1.473  loss_dice_0: 3.144  loss_vq: 1.914  loss_vq_0: 1.914    time: 0.2958  last_time: 0.2908  data_time: 0.0052  last_data_time: 0.0049   lr: 9.5581e-05  max_mem: 7622M
[12/02 00:47:44 d2.utils.events]:  eta: 4:37:25  iter: 2959  total_loss: 15.07  loss_ce: 0.5208  loss_mask: 1.868  loss_dice: 2.986  loss_ce_0: 0.5241  loss_mask_0: 1.864  loss_dice_0: 2.982  loss_vq: 2.241  loss_vq_0: 2.241    time: 0.2958  last_time: 0.2875  data_time: 0.0046  last_data_time: 0.0031   lr: 9.555e-05  max_mem: 7622M
[12/02 00:47:50 d2.utils.events]:  eta: 4:37:15  iter: 2979  total_loss: 15.28  loss_ce: 0.6351  loss_mask: 1.617  loss_dice: 3.155  loss_ce_0: 0.6484  loss_mask_0: 1.589  loss_dice_0: 3.212  loss_vq: 2.105  loss_vq_0: 2.105    time: 0.2958  last_time: 0.2877  data_time: 0.0049  last_data_time: 0.0046   lr: 9.552e-05  max_mem: 7622M
[12/02 00:47:56 d2.utils.events]:  eta: 4:37:08  iter: 2999  total_loss: 14.23  loss_ce: 0.5123  loss_mask: 1.566  loss_dice: 2.851  loss_ce_0: 0.4891  loss_mask_0: 1.645  loss_dice_0: 2.841  loss_vq: 2.119  loss_vq_0: 2.119    time: 0.2958  last_time: 0.2905  data_time: 0.0048  last_data_time: 0.0049   lr: 9.549e-05  max_mem: 7622M
[12/02 00:48:02 d2.utils.events]:  eta: 4:37:03  iter: 3019  total_loss: 15.9  loss_ce: 0.57  loss_mask: 1.937  loss_dice: 3.093  loss_ce_0: 0.6  loss_mask_0: 1.983  loss_dice_0: 3.11  loss_vq: 2.298  loss_vq_0: 2.298    time: 0.2958  last_time: 0.2839  data_time: 0.0055  last_data_time: 0.0016   lr: 9.546e-05  max_mem: 7622M
[12/02 00:48:08 d2.utils.events]:  eta: 4:36:58  iter: 3039  total_loss: 15.17  loss_ce: 0.4626  loss_mask: 1.749  loss_dice: 2.976  loss_ce_0: 0.4109  loss_mask_0: 1.788  loss_dice_0: 3.022  loss_vq: 2.279  loss_vq_0: 2.279    time: 0.2957  last_time: 0.2938  data_time: 0.0050  last_data_time: 0.0042   lr: 9.543e-05  max_mem: 7622M
[12/02 00:48:14 d2.utils.events]:  eta: 4:36:52  iter: 3059  total_loss: 15.05  loss_ce: 0.5705  loss_mask: 1.598  loss_dice: 3.094  loss_ce_0: 0.5902  loss_mask_0: 1.625  loss_dice_0: 3.084  loss_vq: 2.29  loss_vq_0: 2.29    time: 0.2957  last_time: 0.2907  data_time: 0.0055  last_data_time: 0.0054   lr: 9.54e-05  max_mem: 7622M
[12/02 00:48:20 d2.utils.events]:  eta: 4:36:46  iter: 3079  total_loss: 15.37  loss_ce: 0.5054  loss_mask: 1.843  loss_dice: 2.885  loss_ce_0: 0.4815  loss_mask_0: 1.841  loss_dice_0: 2.917  loss_vq: 2.282  loss_vq_0: 2.282    time: 0.2957  last_time: 0.2876  data_time: 0.0052  last_data_time: 0.0033   lr: 9.5369e-05  max_mem: 7622M
[12/02 00:48:26 d2.utils.events]:  eta: 4:36:40  iter: 3099  total_loss: 14.92  loss_ce: 0.5015  loss_mask: 1.789  loss_dice: 3.055  loss_ce_0: 0.5108  loss_mask_0: 1.792  loss_dice_0: 3.046  loss_vq: 2.117  loss_vq_0: 2.117    time: 0.2957  last_time: 0.2881  data_time: 0.0054  last_data_time: 0.0046   lr: 9.5339e-05  max_mem: 7622M
[12/02 00:48:32 d2.utils.events]:  eta: 4:36:34  iter: 3119  total_loss: 15.15  loss_ce: 0.3915  loss_mask: 1.889  loss_dice: 2.81  loss_ce_0: 0.3985  loss_mask_0: 1.887  loss_dice_0: 2.868  loss_vq: 2.309  loss_vq_0: 2.309    time: 0.2957  last_time: 0.2955  data_time: 0.0055  last_data_time: 0.0050   lr: 9.5309e-05  max_mem: 7622M
[12/02 00:48:38 d2.utils.events]:  eta: 4:36:26  iter: 3139  total_loss: 15.17  loss_ce: 0.5909  loss_mask: 1.759  loss_dice: 3.099  loss_ce_0: 0.593  loss_mask_0: 1.751  loss_dice_0: 3.124  loss_vq: 2.031  loss_vq_0: 2.031    time: 0.2957  last_time: 0.2873  data_time: 0.0052  last_data_time: 0.0039   lr: 9.5279e-05  max_mem: 7622M
[12/02 00:48:44 d2.utils.events]:  eta: 4:36:20  iter: 3159  total_loss: 14.88  loss_ce: 0.4134  loss_mask: 1.789  loss_dice: 2.84  loss_ce_0: 0.412  loss_mask_0: 1.689  loss_dice_0: 2.843  loss_vq: 2.148  loss_vq_0: 2.148    time: 0.2957  last_time: 0.3112  data_time: 0.0048  last_data_time: 0.0038   lr: 9.5249e-05  max_mem: 7622M
[12/02 00:48:50 d2.utils.events]:  eta: 4:36:11  iter: 3179  total_loss: 15.1  loss_ce: 0.4926  loss_mask: 1.804  loss_dice: 3.074  loss_ce_0: 0.4751  loss_mask_0: 1.859  loss_dice_0: 3.072  loss_vq: 2.347  loss_vq_0: 2.347    time: 0.2957  last_time: 0.2893  data_time: 0.0054  last_data_time: 0.0044   lr: 9.5219e-05  max_mem: 7622M
[12/02 00:48:56 d2.utils.events]:  eta: 4:36:03  iter: 3199  total_loss: 15.28  loss_ce: 0.5198  loss_mask: 1.641  loss_dice: 2.996  loss_ce_0: 0.4759  loss_mask_0: 1.639  loss_dice_0: 2.978  loss_vq: 2.192  loss_vq_0: 2.192    time: 0.2957  last_time: 0.2913  data_time: 0.0051  last_data_time: 0.0059   lr: 9.5188e-05  max_mem: 7622M
[12/02 00:49:02 d2.utils.events]:  eta: 4:35:59  iter: 3219  total_loss: 15.21  loss_ce: 0.4614  loss_mask: 1.754  loss_dice: 3.097  loss_ce_0: 0.4332  loss_mask_0: 1.723  loss_dice_0: 3.096  loss_vq: 2.261  loss_vq_0: 2.261    time: 0.2956  last_time: 0.2971  data_time: 0.0051  last_data_time: 0.0045   lr: 9.5158e-05  max_mem: 7622M
[12/02 00:49:08 d2.utils.events]:  eta: 4:35:59  iter: 3239  total_loss: 15.47  loss_ce: 0.6215  loss_mask: 1.663  loss_dice: 3.088  loss_ce_0: 0.5997  loss_mask_0: 1.515  loss_dice_0: 3.048  loss_vq: 2.412  loss_vq_0: 2.412    time: 0.2957  last_time: 0.3268  data_time: 0.0057  last_data_time: 0.0056   lr: 9.5128e-05  max_mem: 7622M
[12/02 00:49:14 d2.utils.events]:  eta: 4:35:55  iter: 3259  total_loss: 15.37  loss_ce: 0.5137  loss_mask: 1.611  loss_dice: 2.943  loss_ce_0: 0.4924  loss_mask_0: 1.658  loss_dice_0: 2.995  loss_vq: 2.621  loss_vq_0: 2.621    time: 0.2957  last_time: 0.2937  data_time: 0.0059  last_data_time: 0.0050   lr: 9.5098e-05  max_mem: 7622M
[12/02 00:49:20 d2.utils.events]:  eta: 4:35:48  iter: 3279  total_loss: 15.48  loss_ce: 0.4672  loss_mask: 1.831  loss_dice: 3.131  loss_ce_0: 0.459  loss_mask_0: 1.795  loss_dice_0: 3.254  loss_vq: 2.287  loss_vq_0: 2.287    time: 0.2957  last_time: 0.2901  data_time: 0.0055  last_data_time: 0.0046   lr: 9.5068e-05  max_mem: 7622M
[12/02 00:49:26 d2.utils.events]:  eta: 4:35:47  iter: 3299  total_loss: 15.45  loss_ce: 0.599  loss_mask: 1.643  loss_dice: 3.094  loss_ce_0: 0.5884  loss_mask_0: 1.664  loss_dice_0: 3.124  loss_vq: 2.279  loss_vq_0: 2.279    time: 0.2957  last_time: 0.3283  data_time: 0.0052  last_data_time: 0.0067   lr: 9.5038e-05  max_mem: 7622M
[12/02 00:49:32 d2.utils.events]:  eta: 4:35:40  iter: 3319  total_loss: 15.01  loss_ce: 0.5383  loss_mask: 1.713  loss_dice: 3.091  loss_ce_0: 0.541  loss_mask_0: 1.738  loss_dice_0: 2.975  loss_vq: 2.172  loss_vq_0: 2.172    time: 0.2957  last_time: 0.2988  data_time: 0.0054  last_data_time: 0.0099   lr: 9.5007e-05  max_mem: 7622M
[12/02 00:49:38 d2.utils.events]:  eta: 4:35:35  iter: 3339  total_loss: 15.51  loss_ce: 0.4561  loss_mask: 1.762  loss_dice: 3.139  loss_ce_0: 0.4627  loss_mask_0: 1.802  loss_dice_0: 2.972  loss_vq: 2.385  loss_vq_0: 2.385    time: 0.2958  last_time: 0.2923  data_time: 0.0059  last_data_time: 0.0075   lr: 9.4977e-05  max_mem: 7622M
[12/02 00:49:44 d2.utils.events]:  eta: 4:35:27  iter: 3359  total_loss: 15.69  loss_ce: 0.4901  loss_mask: 1.725  loss_dice: 3.001  loss_ce_0: 0.4688  loss_mask_0: 1.765  loss_dice_0: 2.942  loss_vq: 2.509  loss_vq_0: 2.509    time: 0.2958  last_time: 0.2891  data_time: 0.0046  last_data_time: 0.0033   lr: 9.4947e-05  max_mem: 7622M
[12/02 00:49:50 d2.utils.events]:  eta: 4:35:23  iter: 3379  total_loss: 15.75  loss_ce: 0.4695  loss_mask: 1.844  loss_dice: 2.978  loss_ce_0: 0.4693  loss_mask_0: 1.891  loss_dice_0: 2.969  loss_vq: 2.513  loss_vq_0: 2.513    time: 0.2958  last_time: 0.2915  data_time: 0.0051  last_data_time: 0.0048   lr: 9.4917e-05  max_mem: 7622M
[12/02 00:49:56 d2.utils.events]:  eta: 4:35:13  iter: 3399  total_loss: 15.49  loss_ce: 0.4843  loss_mask: 1.73  loss_dice: 2.919  loss_ce_0: 0.4872  loss_mask_0: 1.817  loss_dice_0: 3  loss_vq: 2.584  loss_vq_0: 2.584    time: 0.2957  last_time: 0.3071  data_time: 0.0050  last_data_time: 0.0037   lr: 9.4887e-05  max_mem: 7622M
[12/02 00:50:03 d2.utils.events]:  eta: 4:35:10  iter: 3419  total_loss: 15.01  loss_ce: 0.4805  loss_mask: 1.652  loss_dice: 3.014  loss_ce_0: 0.491  loss_mask_0: 1.61  loss_dice_0: 3.014  loss_vq: 2.366  loss_vq_0: 2.366    time: 0.2957  last_time: 0.2856  data_time: 0.0055  last_data_time: 0.0034   lr: 9.4857e-05  max_mem: 7622M
[12/02 00:50:09 d2.utils.events]:  eta: 4:35:06  iter: 3439  total_loss: 15.68  loss_ce: 0.5884  loss_mask: 1.603  loss_dice: 3.148  loss_ce_0: 0.5863  loss_mask_0: 1.564  loss_dice_0: 3.043  loss_vq: 2.619  loss_vq_0: 2.619    time: 0.2957  last_time: 0.2949  data_time: 0.0054  last_data_time: 0.0084   lr: 9.4826e-05  max_mem: 7622M
[12/02 00:50:15 d2.utils.events]:  eta: 4:35:01  iter: 3459  total_loss: 15.72  loss_ce: 0.5293  loss_mask: 1.715  loss_dice: 2.968  loss_ce_0: 0.4885  loss_mask_0: 1.721  loss_dice_0: 3.039  loss_vq: 2.312  loss_vq_0: 2.312    time: 0.2957  last_time: 0.2932  data_time: 0.0051  last_data_time: 0.0063   lr: 9.4796e-05  max_mem: 7622M
[12/02 00:50:21 d2.utils.events]:  eta: 4:34:56  iter: 3479  total_loss: 15.24  loss_ce: 0.5162  loss_mask: 1.655  loss_dice: 2.981  loss_ce_0: 0.5245  loss_mask_0: 1.496  loss_dice_0: 2.996  loss_vq: 2.367  loss_vq_0: 2.367    time: 0.2957  last_time: 0.2999  data_time: 0.0054  last_data_time: 0.0087   lr: 9.4766e-05  max_mem: 7622M
[12/02 00:50:27 d2.utils.events]:  eta: 4:34:49  iter: 3499  total_loss: 16.06  loss_ce: 0.5234  loss_mask: 1.764  loss_dice: 3.022  loss_ce_0: 0.532  loss_mask_0: 1.742  loss_dice_0: 2.957  loss_vq: 2.593  loss_vq_0: 2.593    time: 0.2957  last_time: 0.2929  data_time: 0.0054  last_data_time: 0.0063   lr: 9.4736e-05  max_mem: 7622M
[12/02 00:50:33 d2.utils.events]:  eta: 4:34:44  iter: 3519  total_loss: 15.52  loss_ce: 0.5804  loss_mask: 1.656  loss_dice: 3.01  loss_ce_0: 0.5596  loss_mask_0: 1.701  loss_dice_0: 2.921  loss_vq: 2.332  loss_vq_0: 2.332    time: 0.2957  last_time: 0.2920  data_time: 0.0056  last_data_time: 0.0064   lr: 9.4706e-05  max_mem: 7622M
[12/02 00:50:39 d2.utils.events]:  eta: 4:34:39  iter: 3539  total_loss: 16.14  loss_ce: 0.3841  loss_mask: 1.834  loss_dice: 2.84  loss_ce_0: 0.4053  loss_mask_0: 1.793  loss_dice_0: 2.887  loss_vq: 2.686  loss_vq_0: 2.686    time: 0.2957  last_time: 0.2912  data_time: 0.0054  last_data_time: 0.0037   lr: 9.4675e-05  max_mem: 7622M
[12/02 00:50:45 d2.utils.events]:  eta: 4:34:33  iter: 3559  total_loss: 15.66  loss_ce: 0.4587  loss_mask: 1.803  loss_dice: 2.929  loss_ce_0: 0.437  loss_mask_0: 1.803  loss_dice_0: 2.888  loss_vq: 2.663  loss_vq_0: 2.663    time: 0.2957  last_time: 0.2915  data_time: 0.0053  last_data_time: 0.0055   lr: 9.4645e-05  max_mem: 7622M
[12/02 00:50:51 d2.utils.events]:  eta: 4:34:32  iter: 3579  total_loss: 14.97  loss_ce: 0.5058  loss_mask: 1.876  loss_dice: 2.842  loss_ce_0: 0.4863  loss_mask_0: 1.839  loss_dice_0: 2.825  loss_vq: 2.449  loss_vq_0: 2.449    time: 0.2957  last_time: 0.2985  data_time: 0.0054  last_data_time: 0.0094   lr: 9.4615e-05  max_mem: 7622M
[12/02 00:50:57 d2.utils.events]:  eta: 4:34:28  iter: 3599  total_loss: 14.67  loss_ce: 0.5373  loss_mask: 1.43  loss_dice: 2.978  loss_ce_0: 0.5758  loss_mask_0: 1.394  loss_dice_0: 3.027  loss_vq: 2.367  loss_vq_0: 2.367    time: 0.2956  last_time: 0.2901  data_time: 0.0052  last_data_time: 0.0037   lr: 9.4585e-05  max_mem: 7622M
[12/02 00:51:03 d2.utils.events]:  eta: 4:34:22  iter: 3619  total_loss: 15.99  loss_ce: 0.5321  loss_mask: 1.832  loss_dice: 3.128  loss_ce_0: 0.5098  loss_mask_0: 1.862  loss_dice_0: 3.171  loss_vq: 2.618  loss_vq_0: 2.618    time: 0.2956  last_time: 0.2888  data_time: 0.0055  last_data_time: 0.0036   lr: 9.4555e-05  max_mem: 7622M
[12/02 00:51:09 d2.utils.events]:  eta: 4:34:15  iter: 3639  total_loss: 15.5  loss_ce: 0.5768  loss_mask: 1.622  loss_dice: 3.283  loss_ce_0: 0.5616  loss_mask_0: 1.513  loss_dice_0: 3.312  loss_vq: 2.279  loss_vq_0: 2.279    time: 0.2956  last_time: 0.2914  data_time: 0.0050  last_data_time: 0.0043   lr: 9.4525e-05  max_mem: 7622M
[12/02 00:51:15 d2.utils.events]:  eta: 4:34:09  iter: 3659  total_loss: 15.86  loss_ce: 0.6318  loss_mask: 1.579  loss_dice: 3.058  loss_ce_0: 0.6299  loss_mask_0: 1.528  loss_dice_0: 3.087  loss_vq: 2.452  loss_vq_0: 2.452    time: 0.2957  last_time: 0.5413  data_time: 0.0053  last_data_time: 0.0046   lr: 9.4494e-05  max_mem: 7627M
[12/02 00:51:21 d2.utils.events]:  eta: 4:33:58  iter: 3679  total_loss: 15.73  loss_ce: 0.463  loss_mask: 1.84  loss_dice: 2.9  loss_ce_0: 0.4439  loss_mask_0: 1.877  loss_dice_0: 2.889  loss_vq: 2.497  loss_vq_0: 2.497    time: 0.2956  last_time: 0.2883  data_time: 0.0050  last_data_time: 0.0043   lr: 9.4464e-05  max_mem: 7627M
[12/02 00:51:27 d2.utils.events]:  eta: 4:33:47  iter: 3699  total_loss: 16.25  loss_ce: 0.4932  loss_mask: 1.648  loss_dice: 3.149  loss_ce_0: 0.476  loss_mask_0: 1.664  loss_dice_0: 3.211  loss_vq: 2.658  loss_vq_0: 2.658    time: 0.2956  last_time: 0.2866  data_time: 0.0047  last_data_time: 0.0033   lr: 9.4434e-05  max_mem: 7627M
[12/02 00:51:32 d2.utils.events]:  eta: 4:33:41  iter: 3719  total_loss: 16.5  loss_ce: 0.4311  loss_mask: 1.778  loss_dice: 2.979  loss_ce_0: 0.4357  loss_mask_0: 1.755  loss_dice_0: 2.953  loss_vq: 2.776  loss_vq_0: 2.776    time: 0.2956  last_time: 0.2919  data_time: 0.0047  last_data_time: 0.0052   lr: 9.4404e-05  max_mem: 7627M
[12/02 00:51:38 d2.utils.events]:  eta: 4:33:37  iter: 3739  total_loss: 15.86  loss_ce: 0.512  loss_mask: 1.585  loss_dice: 3.136  loss_ce_0: 0.5604  loss_mask_0: 1.556  loss_dice_0: 3.203  loss_vq: 2.657  loss_vq_0: 2.657    time: 0.2956  last_time: 0.2913  data_time: 0.0054  last_data_time: 0.0049   lr: 9.4374e-05  max_mem: 7627M
[12/02 00:51:44 d2.utils.events]:  eta: 4:33:31  iter: 3759  total_loss: 15.84  loss_ce: 0.5444  loss_mask: 1.746  loss_dice: 3.055  loss_ce_0: 0.5435  loss_mask_0: 1.679  loss_dice_0: 3.022  loss_vq: 2.645  loss_vq_0: 2.645    time: 0.2956  last_time: 0.2898  data_time: 0.0052  last_data_time: 0.0059   lr: 9.4343e-05  max_mem: 7627M
[12/02 00:51:50 d2.utils.events]:  eta: 4:33:27  iter: 3779  total_loss: 15.24  loss_ce: 0.3753  loss_mask: 1.74  loss_dice: 2.906  loss_ce_0: 0.3839  loss_mask_0: 1.652  loss_dice_0: 2.857  loss_vq: 2.654  loss_vq_0: 2.654    time: 0.2955  last_time: 0.2876  data_time: 0.0050  last_data_time: 0.0034   lr: 9.4313e-05  max_mem: 7627M
[12/02 00:51:56 d2.utils.events]:  eta: 4:33:22  iter: 3799  total_loss: 15.87  loss_ce: 0.5712  loss_mask: 1.728  loss_dice: 3.219  loss_ce_0: 0.5532  loss_mask_0: 1.69  loss_dice_0: 3.249  loss_vq: 2.732  loss_vq_0: 2.732    time: 0.2955  last_time: 0.2985  data_time: 0.0055  last_data_time: 0.0105   lr: 9.4283e-05  max_mem: 7627M
[12/02 00:52:02 d2.utils.events]:  eta: 4:33:16  iter: 3819  total_loss: 15.7  loss_ce: 0.4865  loss_mask: 1.621  loss_dice: 2.937  loss_ce_0: 0.4639  loss_mask_0: 1.617  loss_dice_0: 2.939  loss_vq: 2.704  loss_vq_0: 2.704    time: 0.2955  last_time: 0.2953  data_time: 0.0052  last_data_time: 0.0045   lr: 9.4253e-05  max_mem: 7627M
[12/02 00:52:08 d2.utils.events]:  eta: 4:33:06  iter: 3839  total_loss: 15.83  loss_ce: 0.5231  loss_mask: 1.733  loss_dice: 2.912  loss_ce_0: 0.5332  loss_mask_0: 1.784  loss_dice_0: 3.005  loss_vq: 2.748  loss_vq_0: 2.748    time: 0.2955  last_time: 0.2876  data_time: 0.0049  last_data_time: 0.0038   lr: 9.4223e-05  max_mem: 7627M
[12/02 00:52:14 d2.utils.events]:  eta: 4:33:04  iter: 3859  total_loss: 16.64  loss_ce: 0.5161  loss_mask: 1.897  loss_dice: 3.091  loss_ce_0: 0.5151  loss_mask_0: 1.831  loss_dice_0: 3.038  loss_vq: 2.677  loss_vq_0: 2.677    time: 0.2955  last_time: 0.3231  data_time: 0.0057  last_data_time: 0.0065   lr: 9.4192e-05  max_mem: 7627M
wandb: Network error (TransientError), entering retry loop.
[12/02 00:52:20 d2.utils.events]:  eta: 4:33:00  iter: 3879  total_loss: 16.32  loss_ce: 0.6033  loss_mask: 1.583  loss_dice: 3.184  loss_ce_0: 0.5906  loss_mask_0: 1.555  loss_dice_0: 3.054  loss_vq: 2.695  loss_vq_0: 2.695    time: 0.2955  last_time: 0.2931  data_time: 0.0051  last_data_time: 0.0049   lr: 9.4162e-05  max_mem: 7627M
[12/02 00:52:26 d2.utils.events]:  eta: 4:32:58  iter: 3899  total_loss: 15.59  loss_ce: 0.5163  loss_mask: 1.488  loss_dice: 2.881  loss_ce_0: 0.5152  loss_mask_0: 1.481  loss_dice_0: 2.974  loss_vq: 2.655  loss_vq_0: 2.655    time: 0.2955  last_time: 0.2930  data_time: 0.0053  last_data_time: 0.0036   lr: 9.4132e-05  max_mem: 7627M
[12/02 00:52:32 d2.utils.events]:  eta: 4:32:53  iter: 3919  total_loss: 16.56  loss_ce: 0.5295  loss_mask: 1.65  loss_dice: 3.204  loss_ce_0: 0.5295  loss_mask_0: 1.741  loss_dice_0: 3.264  loss_vq: 2.443  loss_vq_0: 2.443    time: 0.2955  last_time: 0.2914  data_time: 0.0050  last_data_time: 0.0055   lr: 9.4102e-05  max_mem: 7627M
[12/02 00:52:38 d2.utils.events]:  eta: 4:32:47  iter: 3939  total_loss: 15.4  loss_ce: 0.504  loss_mask: 1.651  loss_dice: 2.863  loss_ce_0: 0.5004  loss_mask_0: 1.702  loss_dice_0: 2.964  loss_vq: 2.443  loss_vq_0: 2.443    time: 0.2955  last_time: 0.2901  data_time: 0.0051  last_data_time: 0.0046   lr: 9.4072e-05  max_mem: 7627M
[12/02 00:52:44 d2.utils.events]:  eta: 4:32:45  iter: 3959  total_loss: 15.68  loss_ce: 0.4988  loss_mask: 1.638  loss_dice: 2.897  loss_ce_0: 0.4902  loss_mask_0: 1.585  loss_dice_0: 2.937  loss_vq: 2.602  loss_vq_0: 2.602    time: 0.2955  last_time: 0.2984  data_time: 0.0051  last_data_time: 0.0078   lr: 9.4041e-05  max_mem: 7627M
[12/02 00:52:50 d2.utils.events]:  eta: 4:32:44  iter: 3979  total_loss: 15.6  loss_ce: 0.4116  loss_mask: 1.622  loss_dice: 2.911  loss_ce_0: 0.4207  loss_mask_0: 1.573  loss_dice_0: 2.917  loss_vq: 2.407  loss_vq_0: 2.407    time: 0.2955  last_time: 0.2924  data_time: 0.0055  last_data_time: 0.0034   lr: 9.4011e-05  max_mem: 7627M
[12/02 00:52:57 d2.utils.events]:  eta: 4:32:40  iter: 3999  total_loss: 16.37  loss_ce: 0.5773  loss_mask: 1.648  loss_dice: 3.004  loss_ce_0: 0.5874  loss_mask_0: 1.643  loss_dice_0: 2.971  loss_vq: 2.719  loss_vq_0: 2.719    time: 0.2955  last_time: 0.2981  data_time: 0.0060  last_data_time: 0.0091   lr: 9.3981e-05  max_mem: 7627M
[12/02 00:53:03 d2.utils.events]:  eta: 4:32:33  iter: 4019  total_loss: 16.12  loss_ce: 0.4832  loss_mask: 1.696  loss_dice: 2.885  loss_ce_0: 0.4849  loss_mask_0: 1.668  loss_dice_0: 2.811  loss_vq: 2.775  loss_vq_0: 2.775    time: 0.2955  last_time: 0.3120  data_time: 0.0048  last_data_time: 0.0045   lr: 9.3951e-05  max_mem: 7627M
[12/02 00:53:09 d2.utils.events]:  eta: 4:32:28  iter: 4039  total_loss: 15.38  loss_ce: 0.5395  loss_mask: 1.634  loss_dice: 3.055  loss_ce_0: 0.5502  loss_mask_0: 1.742  loss_dice_0: 3.079  loss_vq: 2.647  loss_vq_0: 2.647    time: 0.2956  last_time: 0.2857  data_time: 0.0059  last_data_time: 0.0032   lr: 9.3921e-05  max_mem: 7627M
[12/02 00:53:15 d2.utils.events]:  eta: 4:32:21  iter: 4059  total_loss: 15.26  loss_ce: 0.5051  loss_mask: 1.8  loss_dice: 2.824  loss_ce_0: 0.5143  loss_mask_0: 1.769  loss_dice_0: 2.874  loss_vq: 2.633  loss_vq_0: 2.633    time: 0.2956  last_time: 0.2882  data_time: 0.0054  last_data_time: 0.0031   lr: 9.389e-05  max_mem: 7627M
[12/02 00:53:21 d2.utils.events]:  eta: 4:32:14  iter: 4079  total_loss: 15.4  loss_ce: 0.4385  loss_mask: 1.517  loss_dice: 2.931  loss_ce_0: 0.4255  loss_mask_0: 1.521  loss_dice_0: 2.908  loss_vq: 3.024  loss_vq_0: 3.024    time: 0.2955  last_time: 0.2889  data_time: 0.0051  last_data_time: 0.0045   lr: 9.386e-05  max_mem: 7627M
[12/02 00:53:27 d2.utils.events]:  eta: 4:32:04  iter: 4099  total_loss: 15.99  loss_ce: 0.4735  loss_mask: 1.742  loss_dice: 2.819  loss_ce_0: 0.4681  loss_mask_0: 1.641  loss_dice_0: 2.837  loss_vq: 2.792  loss_vq_0: 2.792    time: 0.2955  last_time: 0.2902  data_time: 0.0051  last_data_time: 0.0058   lr: 9.383e-05  max_mem: 7627M
[12/02 00:53:33 d2.utils.events]:  eta: 4:31:55  iter: 4119  total_loss: 15.86  loss_ce: 0.4267  loss_mask: 1.676  loss_dice: 3.088  loss_ce_0: 0.4155  loss_mask_0: 1.648  loss_dice_0: 3.031  loss_vq: 2.562  loss_vq_0: 2.562    time: 0.2955  last_time: 0.3227  data_time: 0.0047  last_data_time: 0.0047   lr: 9.38e-05  max_mem: 7627M
[12/02 00:53:39 d2.utils.events]:  eta: 4:31:49  iter: 4139  total_loss: 15.63  loss_ce: 0.4921  loss_mask: 1.644  loss_dice: 2.767  loss_ce_0: 0.4936  loss_mask_0: 1.649  loss_dice_0: 2.783  loss_vq: 2.874  loss_vq_0: 2.874    time: 0.2955  last_time: 0.2953  data_time: 0.0050  last_data_time: 0.0076   lr: 9.377e-05  max_mem: 7627M
[12/02 00:53:45 d2.utils.events]:  eta: 4:31:43  iter: 4159  total_loss: 15.36  loss_ce: 0.5225  loss_mask: 1.606  loss_dice: 2.697  loss_ce_0: 0.5457  loss_mask_0: 1.669  loss_dice_0: 2.663  loss_vq: 2.962  loss_vq_0: 2.962    time: 0.2955  last_time: 0.2870  data_time: 0.0046  last_data_time: 0.0030   lr: 9.3739e-05  max_mem: 7627M
[12/02 00:53:51 d2.utils.events]:  eta: 4:31:40  iter: 4179  total_loss: 16.53  loss_ce: 0.4937  loss_mask: 1.744  loss_dice: 3.109  loss_ce_0: 0.4987  loss_mask_0: 1.731  loss_dice_0: 3.075  loss_vq: 2.945  loss_vq_0: 2.945    time: 0.2955  last_time: 0.2871  data_time: 0.0051  last_data_time: 0.0015   lr: 9.3709e-05  max_mem: 7627M
[12/02 00:53:57 d2.utils.events]:  eta: 4:31:35  iter: 4199  total_loss: 16.41  loss_ce: 0.5024  loss_mask: 1.777  loss_dice: 3.026  loss_ce_0: 0.4898  loss_mask_0: 1.807  loss_dice_0: 3.02  loss_vq: 2.929  loss_vq_0: 2.929    time: 0.2955  last_time: 0.2935  data_time: 0.0055  last_data_time: 0.0046   lr: 9.3679e-05  max_mem: 7627M
[12/02 00:54:03 d2.utils.events]:  eta: 4:31:26  iter: 4219  total_loss: 15.63  loss_ce: 0.416  loss_mask: 1.704  loss_dice: 2.87  loss_ce_0: 0.4335  loss_mask_0: 1.682  loss_dice_0: 2.946  loss_vq: 2.636  loss_vq_0: 2.636    time: 0.2955  last_time: 0.2902  data_time: 0.0050  last_data_time: 0.0045   lr: 9.3649e-05  max_mem: 7627M
[12/02 00:54:09 d2.utils.events]:  eta: 4:31:19  iter: 4239  total_loss: 16.15  loss_ce: 0.3768  loss_mask: 1.786  loss_dice: 3.028  loss_ce_0: 0.3893  loss_mask_0: 1.816  loss_dice_0: 3.084  loss_vq: 2.922  loss_vq_0: 2.922    time: 0.2955  last_time: 0.3235  data_time: 0.0056  last_data_time: 0.0070   lr: 9.3618e-05  max_mem: 7627M
[12/02 00:54:15 d2.utils.events]:  eta: 4:31:13  iter: 4259  total_loss: 16.99  loss_ce: 0.4807  loss_mask: 1.748  loss_dice: 2.917  loss_ce_0: 0.4457  loss_mask_0: 1.872  loss_dice_0: 2.98  loss_vq: 3.057  loss_vq_0: 3.057    time: 0.2955  last_time: 0.3088  data_time: 0.0055  last_data_time: 0.0068   lr: 9.3588e-05  max_mem: 7627M
[12/02 00:54:21 d2.utils.events]:  eta: 4:31:06  iter: 4279  total_loss: 16.1  loss_ce: 0.4945  loss_mask: 1.679  loss_dice: 2.909  loss_ce_0: 0.4951  loss_mask_0: 1.638  loss_dice_0: 2.943  loss_vq: 2.802  loss_vq_0: 2.802    time: 0.2955  last_time: 0.2947  data_time: 0.0051  last_data_time: 0.0091   lr: 9.3558e-05  max_mem: 7627M
[12/02 00:54:27 d2.utils.events]:  eta: 4:30:57  iter: 4299  total_loss: 16.46  loss_ce: 0.4314  loss_mask: 1.65  loss_dice: 3.108  loss_ce_0: 0.4201  loss_mask_0: 1.652  loss_dice_0: 3.153  loss_vq: 2.879  loss_vq_0: 2.879    time: 0.2955  last_time: 0.2906  data_time: 0.0051  last_data_time: 0.0058   lr: 9.3528e-05  max_mem: 7627M
[12/02 00:54:33 d2.utils.events]:  eta: 4:30:51  iter: 4319  total_loss: 16.47  loss_ce: 0.4261  loss_mask: 1.588  loss_dice: 2.831  loss_ce_0: 0.4257  loss_mask_0: 1.629  loss_dice_0: 2.959  loss_vq: 3.077  loss_vq_0: 3.077    time: 0.2955  last_time: 0.2877  data_time: 0.0052  last_data_time: 0.0053   lr: 9.3498e-05  max_mem: 7627M
[12/02 00:54:39 d2.utils.events]:  eta: 4:30:40  iter: 4339  total_loss: 15.6  loss_ce: 0.5879  loss_mask: 1.542  loss_dice: 2.852  loss_ce_0: 0.5695  loss_mask_0: 1.578  loss_dice_0: 2.859  loss_vq: 2.799  loss_vq_0: 2.799    time: 0.2955  last_time: 0.2999  data_time: 0.0054  last_data_time: 0.0118   lr: 9.3467e-05  max_mem: 7627M
[12/02 00:54:45 d2.utils.events]:  eta: 4:30:34  iter: 4359  total_loss: 16.18  loss_ce: 0.4235  loss_mask: 1.701  loss_dice: 2.877  loss_ce_0: 0.4205  loss_mask_0: 1.613  loss_dice_0: 2.903  loss_vq: 2.749  loss_vq_0: 2.749    time: 0.2955  last_time: 0.2928  data_time: 0.0051  last_data_time: 0.0049   lr: 9.3437e-05  max_mem: 7627M
[12/02 00:54:51 d2.utils.events]:  eta: 4:30:24  iter: 4379  total_loss: 16.31  loss_ce: 0.4968  loss_mask: 1.757  loss_dice: 2.892  loss_ce_0: 0.4896  loss_mask_0: 1.783  loss_dice_0: 2.897  loss_vq: 3.186  loss_vq_0: 3.186    time: 0.2955  last_time: 0.2948  data_time: 0.0050  last_data_time: 0.0051   lr: 9.3407e-05  max_mem: 7627M
[12/02 00:54:57 d2.utils.events]:  eta: 4:30:22  iter: 4399  total_loss: 16.45  loss_ce: 0.4154  loss_mask: 1.731  loss_dice: 2.835  loss_ce_0: 0.4147  loss_mask_0: 1.698  loss_dice_0: 2.972  loss_vq: 2.899  loss_vq_0: 2.899    time: 0.2955  last_time: 0.2947  data_time: 0.0048  last_data_time: 0.0053   lr: 9.3377e-05  max_mem: 7627M
[12/02 00:55:03 d2.utils.events]:  eta: 4:30:16  iter: 4419  total_loss: 16.14  loss_ce: 0.4253  loss_mask: 1.792  loss_dice: 2.808  loss_ce_0: 0.4207  loss_mask_0: 1.802  loss_dice_0: 2.872  loss_vq: 2.827  loss_vq_0: 2.827    time: 0.2955  last_time: 0.3378  data_time: 0.0057  last_data_time: 0.0085   lr: 9.3346e-05  max_mem: 7627M
[12/02 00:55:09 d2.utils.events]:  eta: 4:30:09  iter: 4439  total_loss: 16.97  loss_ce: 0.5064  loss_mask: 1.734  loss_dice: 3.039  loss_ce_0: 0.5062  loss_mask_0: 1.691  loss_dice_0: 3.049  loss_vq: 3.107  loss_vq_0: 3.107    time: 0.2955  last_time: 0.2970  data_time: 0.0054  last_data_time: 0.0082   lr: 9.3316e-05  max_mem: 7627M
[12/02 00:55:15 d2.utils.events]:  eta: 4:30:03  iter: 4459  total_loss: 16.87  loss_ce: 0.4115  loss_mask: 1.779  loss_dice: 2.905  loss_ce_0: 0.4191  loss_mask_0: 1.713  loss_dice_0: 2.848  loss_vq: 3.086  loss_vq_0: 3.086    time: 0.2955  last_time: 0.2904  data_time: 0.0055  last_data_time: 0.0044   lr: 9.3286e-05  max_mem: 7627M
[12/02 00:55:21 d2.utils.events]:  eta: 4:29:59  iter: 4479  total_loss: 16.4  loss_ce: 0.4269  loss_mask: 1.717  loss_dice: 2.963  loss_ce_0: 0.4189  loss_mask_0: 1.762  loss_dice_0: 2.96  loss_vq: 3.167  loss_vq_0: 3.167    time: 0.2955  last_time: 0.2871  data_time: 0.0048  last_data_time: 0.0035   lr: 9.3256e-05  max_mem: 7627M
[12/02 00:55:27 d2.utils.events]:  eta: 4:29:47  iter: 4499  total_loss: 16.12  loss_ce: 0.4255  loss_mask: 1.934  loss_dice: 2.903  loss_ce_0: 0.434  loss_mask_0: 1.98  loss_dice_0: 2.958  loss_vq: 2.938  loss_vq_0: 2.938    time: 0.2955  last_time: 0.3154  data_time: 0.0053  last_data_time: 0.0097   lr: 9.3225e-05  max_mem: 7627M
[12/02 00:55:33 d2.utils.events]:  eta: 4:29:41  iter: 4519  total_loss: 17.14  loss_ce: 0.3966  loss_mask: 1.609  loss_dice: 2.901  loss_ce_0: 0.4024  loss_mask_0: 1.653  loss_dice_0: 2.851  loss_vq: 3.592  loss_vq_0: 3.592    time: 0.2955  last_time: 0.2915  data_time: 0.0051  last_data_time: 0.0044   lr: 9.3195e-05  max_mem: 7627M
[12/02 00:55:39 d2.utils.events]:  eta: 4:29:35  iter: 4539  total_loss: 16.76  loss_ce: 0.3896  loss_mask: 1.698  loss_dice: 2.773  loss_ce_0: 0.4101  loss_mask_0: 1.692  loss_dice_0: 2.835  loss_vq: 3.079  loss_vq_0: 3.079    time: 0.2954  last_time: 0.2838  data_time: 0.0048  last_data_time: 0.0017   lr: 9.3165e-05  max_mem: 7627M
[12/02 00:55:45 d2.utils.events]:  eta: 4:29:33  iter: 4559  total_loss: 16.28  loss_ce: 0.4515  loss_mask: 1.422  loss_dice: 2.846  loss_ce_0: 0.4533  loss_mask_0: 1.476  loss_dice_0: 2.877  loss_vq: 3.146  loss_vq_0: 3.146    time: 0.2954  last_time: 0.2967  data_time: 0.0052  last_data_time: 0.0052   lr: 9.3135e-05  max_mem: 7627M
[12/02 00:55:51 d2.utils.events]:  eta: 4:29:23  iter: 4579  total_loss: 15.75  loss_ce: 0.4226  loss_mask: 1.778  loss_dice: 2.631  loss_ce_0: 0.4161  loss_mask_0: 1.794  loss_dice_0: 2.646  loss_vq: 2.892  loss_vq_0: 2.892    time: 0.2954  last_time: 0.3269  data_time: 0.0052  last_data_time: 0.0071   lr: 9.3105e-05  max_mem: 7627M
[12/02 00:55:57 d2.utils.events]:  eta: 4:29:17  iter: 4599  total_loss: 17.78  loss_ce: 0.5012  loss_mask: 1.675  loss_dice: 2.903  loss_ce_0: 0.5157  loss_mask_0: 1.764  loss_dice_0: 2.879  loss_vq: 3.563  loss_vq_0: 3.563    time: 0.2955  last_time: 0.2935  data_time: 0.0060  last_data_time: 0.0049   lr: 9.3074e-05  max_mem: 7627M
[12/02 00:56:03 d2.utils.events]:  eta: 4:29:09  iter: 4619  total_loss: 16.52  loss_ce: 0.4649  loss_mask: 1.697  loss_dice: 2.839  loss_ce_0: 0.4478  loss_mask_0: 1.688  loss_dice_0: 2.918  loss_vq: 3.187  loss_vq_0: 3.187    time: 0.2954  last_time: 0.2901  data_time: 0.0050  last_data_time: 0.0036   lr: 9.3044e-05  max_mem: 7627M
[12/02 00:56:09 d2.utils.events]:  eta: 4:29:07  iter: 4639  total_loss: 17.54  loss_ce: 0.4654  loss_mask: 1.693  loss_dice: 2.874  loss_ce_0: 0.4638  loss_mask_0: 1.704  loss_dice_0: 2.919  loss_vq: 3.221  loss_vq_0: 3.221    time: 0.2954  last_time: 0.2924  data_time: 0.0055  last_data_time: 0.0048   lr: 9.3014e-05  max_mem: 7627M
[12/02 00:56:15 d2.utils.events]:  eta: 4:29:00  iter: 4659  total_loss: 17.78  loss_ce: 0.5265  loss_mask: 1.657  loss_dice: 2.859  loss_ce_0: 0.5128  loss_mask_0: 1.716  loss_dice_0: 2.813  loss_vq: 3.703  loss_vq_0: 3.703    time: 0.2955  last_time: 0.2899  data_time: 0.0054  last_data_time: 0.0045   lr: 9.2984e-05  max_mem: 7627M
[12/02 00:56:21 d2.utils.events]:  eta: 4:29:00  iter: 4679  total_loss: 16.01  loss_ce: 0.6312  loss_mask: 1.608  loss_dice: 2.98  loss_ce_0: 0.5914  loss_mask_0: 1.568  loss_dice_0: 2.96  loss_vq: 3.196  loss_vq_0: 3.196    time: 0.2955  last_time: 0.2957  data_time: 0.0052  last_data_time: 0.0044   lr: 9.2953e-05  max_mem: 7627M
[12/02 00:56:27 d2.utils.events]:  eta: 4:28:58  iter: 4699  total_loss: 17.1  loss_ce: 0.4767  loss_mask: 1.629  loss_dice: 2.924  loss_ce_0: 0.4558  loss_mask_0: 1.571  loss_dice_0: 2.863  loss_vq: 3.24  loss_vq_0: 3.24    time: 0.2954  last_time: 0.2859  data_time: 0.0049  last_data_time: 0.0032   lr: 9.2923e-05  max_mem: 7627M
[12/02 00:56:33 d2.utils.events]:  eta: 4:28:55  iter: 4719  total_loss: 17.35  loss_ce: 0.5312  loss_mask: 1.606  loss_dice: 3.041  loss_ce_0: 0.5392  loss_mask_0: 1.573  loss_dice_0: 3.031  loss_vq: 3.399  loss_vq_0: 3.399    time: 0.2955  last_time: 0.2974  data_time: 0.0060  last_data_time: 0.0079   lr: 9.2893e-05  max_mem: 7627M
[12/02 00:56:39 d2.utils.events]:  eta: 4:28:48  iter: 4739  total_loss: 17.77  loss_ce: 0.4929  loss_mask: 1.823  loss_dice: 2.891  loss_ce_0: 0.5001  loss_mask_0: 1.757  loss_dice_0: 2.852  loss_vq: 3.422  loss_vq_0: 3.422    time: 0.2955  last_time: 0.3099  data_time: 0.0052  last_data_time: 0.0104   lr: 9.2863e-05  max_mem: 7627M
[12/02 00:56:45 d2.utils.events]:  eta: 4:28:44  iter: 4759  total_loss: 16.35  loss_ce: 0.5594  loss_mask: 1.703  loss_dice: 2.869  loss_ce_0: 0.5309  loss_mask_0: 1.675  loss_dice_0: 2.812  loss_vq: 3.211  loss_vq_0: 3.211    time: 0.2955  last_time: 0.2992  data_time: 0.0053  last_data_time: 0.0054   lr: 9.2832e-05  max_mem: 7627M
[12/02 00:56:51 d2.utils.events]:  eta: 4:28:44  iter: 4779  total_loss: 16.87  loss_ce: 0.5419  loss_mask: 1.663  loss_dice: 3.222  loss_ce_0: 0.5206  loss_mask_0: 1.639  loss_dice_0: 3.18  loss_vq: 3.039  loss_vq_0: 3.039    time: 0.2955  last_time: 0.2968  data_time: 0.0053  last_data_time: 0.0052   lr: 9.2802e-05  max_mem: 7627M
[12/02 00:56:57 d2.utils.events]:  eta: 4:28:42  iter: 4799  total_loss: 17.1  loss_ce: 0.5406  loss_mask: 1.538  loss_dice: 2.926  loss_ce_0: 0.5157  loss_mask_0: 1.492  loss_dice_0: 2.931  loss_vq: 3.237  loss_vq_0: 3.237    time: 0.2955  last_time: 0.2947  data_time: 0.0060  last_data_time: 0.0047   lr: 9.2772e-05  max_mem: 7627M
[12/02 00:57:03 d2.utils.events]:  eta: 4:28:40  iter: 4819  total_loss: 17.44  loss_ce: 0.4052  loss_mask: 1.821  loss_dice: 2.582  loss_ce_0: 0.3969  loss_mask_0: 1.804  loss_dice_0: 2.479  loss_vq: 3.665  loss_vq_0: 3.665    time: 0.2955  last_time: 0.2997  data_time: 0.0051  last_data_time: 0.0094   lr: 9.2742e-05  max_mem: 7627M
[12/02 00:57:09 d2.utils.events]:  eta: 4:28:41  iter: 4839  total_loss: 16.78  loss_ce: 0.5794  loss_mask: 1.67  loss_dice: 2.866  loss_ce_0: 0.5668  loss_mask_0: 1.649  loss_dice_0: 2.938  loss_vq: 3.263  loss_vq_0: 3.263    time: 0.2955  last_time: 0.2894  data_time: 0.0050  last_data_time: 0.0032   lr: 9.2711e-05  max_mem: 7627M
[12/02 00:57:15 d2.utils.events]:  eta: 4:28:38  iter: 4859  total_loss: 16.99  loss_ce: 0.5054  loss_mask: 1.62  loss_dice: 2.688  loss_ce_0: 0.4812  loss_mask_0: 1.671  loss_dice_0: 2.789  loss_vq: 3.506  loss_vq_0: 3.506    time: 0.2955  last_time: 0.2888  data_time: 0.0051  last_data_time: 0.0051   lr: 9.2681e-05  max_mem: 7627M
[12/02 00:57:21 d2.utils.events]:  eta: 4:28:36  iter: 4879  total_loss: 16.32  loss_ce: 0.5906  loss_mask: 1.666  loss_dice: 3.159  loss_ce_0: 0.5711  loss_mask_0: 1.633  loss_dice_0: 3.162  loss_vq: 3.075  loss_vq_0: 3.075    time: 0.2955  last_time: 0.2930  data_time: 0.0052  last_data_time: 0.0054   lr: 9.2651e-05  max_mem: 7627M
[12/02 00:57:27 d2.utils.events]:  eta: 4:28:25  iter: 4899  total_loss: 17.12  loss_ce: 0.4566  loss_mask: 1.757  loss_dice: 2.869  loss_ce_0: 0.4546  loss_mask_0: 1.748  loss_dice_0: 2.803  loss_vq: 3.552  loss_vq_0: 3.552    time: 0.2955  last_time: 0.3225  data_time: 0.0053  last_data_time: 0.0068   lr: 9.2621e-05  max_mem: 7627M
[12/02 00:57:33 d2.utils.events]:  eta: 4:28:22  iter: 4919  total_loss: 16.86  loss_ce: 0.5169  loss_mask: 1.838  loss_dice: 2.759  loss_ce_0: 0.5028  loss_mask_0: 1.814  loss_dice_0: 2.714  loss_vq: 3.349  loss_vq_0: 3.349    time: 0.2955  last_time: 0.2880  data_time: 0.0051  last_data_time: 0.0015   lr: 9.259e-05  max_mem: 7627M
[12/02 00:57:39 d2.utils.events]:  eta: 4:28:16  iter: 4939  total_loss: 17.9  loss_ce: 0.4583  loss_mask: 1.705  loss_dice: 2.747  loss_ce_0: 0.4521  loss_mask_0: 1.659  loss_dice_0: 2.757  loss_vq: 3.806  loss_vq_0: 3.806    time: 0.2955  last_time: 0.2927  data_time: 0.0052  last_data_time: 0.0063   lr: 9.256e-05  max_mem: 7627M
[12/02 00:57:45 d2.utils.events]:  eta: 4:28:07  iter: 4959  total_loss: 17.94  loss_ce: 0.4512  loss_mask: 1.68  loss_dice: 2.75  loss_ce_0: 0.4361  loss_mask_0: 1.676  loss_dice_0: 2.695  loss_vq: 3.889  loss_vq_0: 3.889    time: 0.2955  last_time: 0.2886  data_time: 0.0054  last_data_time: 0.0059   lr: 9.253e-05  max_mem: 7627M
[12/02 00:57:51 d2.utils.events]:  eta: 4:28:02  iter: 4979  total_loss: 16.8  loss_ce: 0.5119  loss_mask: 1.566  loss_dice: 2.766  loss_ce_0: 0.5024  loss_mask_0: 1.591  loss_dice_0: 2.763  loss_vq: 3.321  loss_vq_0: 3.321    time: 0.2955  last_time: 0.3271  data_time: 0.0050  last_data_time: 0.0058   lr: 9.25e-05  max_mem: 7627M
[12/02 00:57:57 fvcore.common.checkpoint]: Saving checkpoint to output/vqsan1024_xattn/model_0004999.pth
[12/02 00:57:59 d2.data.datasets.coco]: Loaded 5000 images with semantic segmentation from datasets/coco/val2017
[12/02 00:57:59 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(640, 640), max_size=2560, sample_style='choice')]
[12/02 00:57:59 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>
[12/02 00:57:59 d2.data.common]: Serializing 5000 elements to byte tensors and concatenating them all ...
[12/02 00:57:59 d2.data.common]: Serialized dataset takes 1.01 MiB
[12/02 00:57:59 d2.data.datasets.coco]: Loaded 5000 images with semantic segmentation from datasets/coco/val2017
[12/02 00:57:59 d2.evaluation.evaluator]: Start inference on 5000 batches
[12/02 00:58:02 d2.utils.memory]: Attempting to copy inputs of <bound method SAN.semantic_inference of SAN(
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_vq': 1.0, 'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_vq_0': 1.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0}
      num_classes: 171
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (query_proj): Linear(in_features=240, out_features=768, bias=False)
  (side_adapter_network): RegionwiseSideAdapterNetwork(
    (vit_model): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 240, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (patch_drop): Identity()
      (norm_pre): Identity()
      (blocks): Sequential(
        (0): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (1): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (2): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (3): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (4): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (5): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (6): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (7): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
      )
      (fc_norm): Identity()
      (head_drop): Dropout(p=0.0, inplace=False)
      (head): Identity()
      (norm): Identity()
    )
    (fusion_layers): ModuleDict(
      (layer_0): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_1): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_2): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_3): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
    )
    (mask_decoder): MLPMaskDecoder(
      (query_mlp): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=240, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (pix_mlp): MLP(
        (layers): ModuleList(
          (0): Conv2d(240, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (attn_mlp): MLP(
        (layers): ModuleList(
          (0): Conv2d(240, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(256, 3072, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (bias_scaling): Linear(in_features=1, out_features=1, bias=True)
    )
  )
  (clip_visual_extractor): FeatureExtractor(
    (patchnorm_pre_ln): Identity()
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (patch_dropout): Identity()
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (3): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (4): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (5): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (6): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (7): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (8): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
    )
  )
  (clip_rec_head): RecWithAttnbiasHead(
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (ov_classifier): LearnableBgOvClassifier(
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (1): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (2): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (3): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (4): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (5): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (6): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (7): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (8): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (9): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (10): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (11): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (token_embedding): Embedding(49408, 512)
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)> to CPU due to CUDA OOM
[12/02 00:58:04 d2.evaluation.evaluator]: Inference done 11/5000. Dataloading: 0.0014 s/iter. Inference: 0.3568 s/iter. Eval: 0.0250 s/iter. Total: 0.3833 s/iter. ETA=0:31:52
[12/02 00:58:04 d2.utils.memory]: Attempting to copy inputs of <bound method SAN.semantic_inference of SAN(
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_vq': 1.0, 'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_vq_0': 1.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0}
      num_classes: 171
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (query_proj): Linear(in_features=240, out_features=768, bias=False)
  (side_adapter_network): RegionwiseSideAdapterNetwork(
    (vit_model): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 240, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (patch_drop): Identity()
      (norm_pre): Identity()
      (blocks): Sequential(
        (0): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (1): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (2): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (3): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (4): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (5): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (6): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (7): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
      )
      (fc_norm): Identity()
      (head_drop): Dropout(p=0.0, inplace=False)
      (head): Identity()
      (norm): Identity()
    )
    (fusion_layers): ModuleDict(
      (layer_0): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_1): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_2): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_3): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
    )
    (mask_decoder): MLPMaskDecoder(
      (query_mlp): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=240, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (pix_mlp): MLP(
        (layers): ModuleList(
          (0): Conv2d(240, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (attn_mlp): MLP(
        (layers): ModuleList(
          (0): Conv2d(240, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(256, 3072, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (bias_scaling): Linear(in_features=1, out_features=1, bias=True)
    )
  )
  (clip_visual_extractor): FeatureExtractor(
    (patchnorm_pre_ln): Identity()
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (patch_dropout): Identity()
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (3): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (4): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (5): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (6): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (7): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (8): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
    )
  )
  (clip_rec_head): RecWithAttnbiasHead(
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (ov_classifier): LearnableBgOvClassifier(
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (1): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (2): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (3): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (4): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (5): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (6): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (7): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (8): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (9): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (10): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (11): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (token_embedding): Embedding(49408, 512)
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)> to CPU due to CUDA OOM
[12/02 00:58:09 d2.evaluation.evaluator]: Inference done 44/5000. Dataloading: 0.0016 s/iter. Inference: 0.1707 s/iter. Eval: 0.0152 s/iter. Total: 0.1877 s/iter. ETA=0:15:30
[12/02 00:58:10 d2.utils.memory]: Attempting to copy inputs of <bound method SAN.semantic_inference of SAN(
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_vq': 1.0, 'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_vq_0': 1.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0}
      num_classes: 171
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (query_proj): Linear(in_features=240, out_features=768, bias=False)
  (side_adapter_network): RegionwiseSideAdapterNetwork(
    (vit_model): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 240, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (patch_drop): Identity()
      (norm_pre): Identity()
      (blocks): Sequential(
        (0): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (1): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (2): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (3): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (4): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (5): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (6): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (7): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
      )
      (fc_norm): Identity()
      (head_drop): Dropout(p=0.0, inplace=False)
      (head): Identity()
      (norm): Identity()
    )
    (fusion_layers): ModuleDict(
      (layer_0): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_1): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_2): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_3): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
    )
    (mask_decoder): MLPMaskDecoder(
      (query_mlp): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=240, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (pix_mlp): MLP(
        (layers): ModuleList(
          (0): Conv2d(240, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (attn_mlp): MLP(
        (layers): ModuleList(
          (0): Conv2d(240, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(256, 3072, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (bias_scaling): Linear(in_features=1, out_features=1, bias=True)
    )
  )
  (clip_visual_extractor): FeatureExtractor(
    (patchnorm_pre_ln): Identity()
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (patch_dropout): Identity()
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (3): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (4): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (5): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (6): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (7): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (8): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
    )
  )
  (clip_rec_head): RecWithAttnbiasHead(
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (ov_classifier): LearnableBgOvClassifier(
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (1): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (2): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (3): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (4): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (5): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (6): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (7): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (8): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (9): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (10): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (11): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (token_embedding): Embedding(49408, 512)
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)> to CPU due to CUDA OOM
[12/02 00:58:14 d2.evaluation.evaluator]: Inference done 82/5000. Dataloading: 0.0017 s/iter. Inference: 0.1450 s/iter. Eval: 0.0144 s/iter. Total: 0.1611 s/iter. ETA=0:13:12
[12/02 00:58:14 d2.utils.memory]: Attempting to copy inputs of <bound method SAN.semantic_inference of SAN(
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_vq': 1.0, 'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_vq_0': 1.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0}
      num_classes: 171
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (query_proj): Linear(in_features=240, out_features=768, bias=False)
  (side_adapter_network): RegionwiseSideAdapterNetwork(
    (vit_model): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 240, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (patch_drop): Identity()
      (norm_pre): Identity()
      (blocks): Sequential(
        (0): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (1): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (2): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (3): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (4): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (5): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (6): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (7): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
      )
      (fc_norm): Identity()
      (head_drop): Dropout(p=0.0, inplace=False)
      (head): Identity()
      (norm): Identity()
    )
    (fusion_layers): ModuleDict(
      (layer_0): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_1): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_2): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_3): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
    )
    (mask_decoder): MLPMaskDecoder(
      (query_mlp): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=240, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (pix_mlp): MLP(
        (layers): ModuleList(
          (0): Conv2d(240, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (attn_mlp): MLP(
        (layers): ModuleList(
          (0): Conv2d(240, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(256, 3072, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (bias_scaling): Linear(in_features=1, out_features=1, bias=True)
    )
  )
  (clip_visual_extractor): FeatureExtractor(
    (patchnorm_pre_ln): Identity()
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (patch_dropout): Identity()
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (3): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (4): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (5): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (6): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (7): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (8): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
    )
  )
  (clip_rec_head): RecWithAttnbiasHead(
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (ov_classifier): LearnableBgOvClassifier(
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (1): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (2): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (3): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (4): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (5): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (6): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (7): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (8): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (9): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (10): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (11): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (token_embedding): Embedding(49408, 512)
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)> to CPU due to CUDA OOM
[12/02 00:58:19 d2.utils.memory]: Attempting to copy inputs of <bound method SAN.semantic_inference of SAN(
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_vq': 1.0, 'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_vq_0': 1.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0}
      num_classes: 171
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (query_proj): Linear(in_features=240, out_features=768, bias=False)
  (side_adapter_network): RegionwiseSideAdapterNetwork(
    (vit_model): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 240, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (patch_drop): Identity()
      (norm_pre): Identity()
      (blocks): Sequential(
        (0): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (1): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (2): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (3): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (4): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (5): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (6): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (7): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
      )
      (fc_norm): Identity()
      (head_drop): Dropout(p=0.0, inplace=False)
      (head): Identity()
      (norm): Identity()
    )
    (fusion_layers): ModuleDict(
      (layer_0): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_1): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_2): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_3): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
    )
    (mask_decoder): MLPMaskDecoder(
      (query_mlp): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=240, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (pix_mlp): MLP(
        (layers): ModuleList(
          (0): Conv2d(240, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (attn_mlp): MLP(
        (layers): ModuleList(
          (0): Conv2d(240, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(256, 3072, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (bias_scaling): Linear(in_features=1, out_features=1, bias=True)
    )
  )
  (clip_visual_extractor): FeatureExtractor(
    (patchnorm_pre_ln): Identity()
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (patch_dropout): Identity()
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (3): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (4): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (5): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (6): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (7): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (8): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
    )
  )
  (clip_rec_head): RecWithAttnbiasHead(
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (ov_classifier): LearnableBgOvClassifier(
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (1): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (2): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (3): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (4): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (5): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (6): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (7): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (8): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (9): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (10): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (11): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (token_embedding): Embedding(49408, 512)
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)> to CPU due to CUDA OOM
[12/02 00:58:20 d2.evaluation.evaluator]: Inference done 114/5000. Dataloading: 0.0017 s/iter. Inference: 0.1544 s/iter. Eval: 0.0151 s/iter. Total: 0.1713 s/iter. ETA=0:13:57
[12/02 00:58:25 d2.evaluation.evaluator]: Inference done 162/5000. Dataloading: 0.0017 s/iter. Inference: 0.1346 s/iter. Eval: 0.0146 s/iter. Total: 0.1509 s/iter. ETA=0:12:10
[12/02 00:58:30 d2.evaluation.evaluator]: Inference done 212/5000. Dataloading: 0.0018 s/iter. Inference: 0.1228 s/iter. Eval: 0.0142 s/iter. Total: 0.1388 s/iter. ETA=0:11:04
[12/02 00:58:30 d2.utils.memory]: Attempting to copy inputs of <bound method SAN.semantic_inference of SAN(
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_vq': 1.0, 'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_vq_0': 1.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0}
      num_classes: 171
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (query_proj): Linear(in_features=240, out_features=768, bias=False)
  (side_adapter_network): RegionwiseSideAdapterNetwork(
    (vit_model): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 240, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (patch_drop): Identity()
      (norm_pre): Identity()
      (blocks): Sequential(
        (0): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (1): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (2): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (3): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (4): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (5): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (6): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (7): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
      )
      (fc_norm): Identity()
      (head_drop): Dropout(p=0.0, inplace=False)
      (head): Identity()
      (norm): Identity()
    )
    (fusion_layers): ModuleDict(
      (layer_0): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_1): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_2): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_3): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
    )
    (mask_decoder): MLPMaskDecoder(
      (query_mlp): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=240, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (pix_mlp): MLP(
        (layers): ModuleList(
          (0): Conv2d(240, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (attn_mlp): MLP(
        (layers): ModuleList(
          (0): Conv2d(240, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(256, 3072, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (bias_scaling): Linear(in_features=1, out_features=1, bias=True)
    )
  )
  (clip_visual_extractor): FeatureExtractor(
    (patchnorm_pre_ln): Identity()
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (patch_dropout): Identity()
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (3): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (4): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (5): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (6): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (7): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (8): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
    )
  )
  (clip_rec_head): RecWithAttnbiasHead(
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (ov_classifier): LearnableBgOvClassifier(
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (1): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (2): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (3): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (4): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (5): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (6): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (7): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (8): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (9): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (10): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (11): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (token_embedding): Embedding(49408, 512)
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)> to CPU due to CUDA OOM
[12/02 00:58:35 d2.evaluation.evaluator]: Inference done 249/5000. Dataloading: 0.0018 s/iter. Inference: 0.1226 s/iter. Eval: 0.0141 s/iter. Total: 0.1386 s/iter. ETA=0:10:58
[12/02 00:58:40 d2.evaluation.evaluator]: Inference done 299/5000. Dataloading: 0.0018 s/iter. Inference: 0.1165 s/iter. Eval: 0.0139 s/iter. Total: 0.1323 s/iter. ETA=0:10:21
[12/02 00:58:44 d2.utils.memory]: Attempting to copy inputs of <bound method SAN.semantic_inference of SAN(
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_vq': 1.0, 'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_vq_0': 1.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0}
      num_classes: 171
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (query_proj): Linear(in_features=240, out_features=768, bias=False)
  (side_adapter_network): RegionwiseSideAdapterNetwork(
    (vit_model): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 240, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (patch_drop): Identity()
      (norm_pre): Identity()
      (blocks): Sequential(
        (0): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (1): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (2): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (3): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (4): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (5): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (6): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (7): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
      )
      (fc_norm): Identity()
      (head_drop): Dropout(p=0.0, inplace=False)
      (head): Identity()
      (norm): Identity()
    )
    (fusion_layers): ModuleDict(
      (layer_0): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_1): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_2): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_3): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
    )
    (mask_decoder): MLPMaskDecoder(
      (query_mlp): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=240, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (pix_mlp): MLP(
        (layers): ModuleList(
          (0): Conv2d(240, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (attn_mlp): MLP(
        (layers): ModuleList(
          (0): Conv2d(240, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(256, 3072, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (bias_scaling): Linear(in_features=1, out_features=1, bias=True)
    )
  )
  (clip_visual_extractor): FeatureExtractor(
    (patchnorm_pre_ln): Identity()
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (patch_dropout): Identity()
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (3): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (4): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (5): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (6): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (7): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (8): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
    )
  )
  (clip_rec_head): RecWithAttnbiasHead(
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (ov_classifier): LearnableBgOvClassifier(
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (1): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (2): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (3): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (4): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (5): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (6): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (7): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (8): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (9): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (10): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (11): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (token_embedding): Embedding(49408, 512)
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)> to CPU due to CUDA OOM
[12/02 00:58:45 d2.evaluation.evaluator]: Inference done 335/5000. Dataloading: 0.0018 s/iter. Inference: 0.1173 s/iter. Eval: 0.0140 s/iter. Total: 0.1332 s/iter. ETA=0:10:21
[12/02 00:58:50 d2.evaluation.evaluator]: Inference done 383/5000. Dataloading: 0.0018 s/iter. Inference: 0.1141 s/iter. Eval: 0.0138 s/iter. Total: 0.1297 s/iter. ETA=0:09:59
[12/02 00:58:51 d2.utils.memory]: Attempting to copy inputs of <bound method SAN.semantic_inference of SAN(
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_vq': 1.0, 'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_vq_0': 1.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0}
      num_classes: 171
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (query_proj): Linear(in_features=240, out_features=768, bias=False)
  (side_adapter_network): RegionwiseSideAdapterNetwork(
    (vit_model): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 240, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (patch_drop): Identity()
      (norm_pre): Identity()
      (blocks): Sequential(
        (0): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (1): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (2): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (3): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (4): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (5): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (6): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (7): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
      )
      (fc_norm): Identity()
      (head_drop): Dropout(p=0.0, inplace=False)
      (head): Identity()
      (norm): Identity()
    )
    (fusion_layers): ModuleDict(
      (layer_0): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_1): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_2): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_3): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
    )
    (mask_decoder): MLPMaskDecoder(
      (query_mlp): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=240, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (pix_mlp): MLP(
        (layers): ModuleList(
          (0): Conv2d(240, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (attn_mlp): MLP(
        (layers): ModuleList(
          (0): Conv2d(240, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(256, 3072, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (bias_scaling): Linear(in_features=1, out_features=1, bias=True)
    )
  )
  (clip_visual_extractor): FeatureExtractor(
    (patchnorm_pre_ln): Identity()
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (patch_dropout): Identity()
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (3): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (4): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (5): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (6): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (7): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (8): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
    )
  )
  (clip_rec_head): RecWithAttnbiasHead(
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (ov_classifier): LearnableBgOvClassifier(
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (1): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (2): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (3): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (4): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (5): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (6): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (7): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (8): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (9): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (10): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (11): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (token_embedding): Embedding(49408, 512)
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)> to CPU due to CUDA OOM
[12/02 00:58:52 d2.utils.memory]: Attempting to copy inputs of <bound method SAN.semantic_inference of SAN(
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_vq': 1.0, 'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_vq_0': 1.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0}
      num_classes: 171
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (query_proj): Linear(in_features=240, out_features=768, bias=False)
  (side_adapter_network): RegionwiseSideAdapterNetwork(
    (vit_model): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 240, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (patch_drop): Identity()
      (norm_pre): Identity()
      (blocks): Sequential(
        (0): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (1): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (2): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (3): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (4): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (5): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (6): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (7): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
      )
      (fc_norm): Identity()
      (head_drop): Dropout(p=0.0, inplace=False)
      (head): Identity()
      (norm): Identity()
    )
    (fusion_layers): ModuleDict(
      (layer_0): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_1): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_2): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_3): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
    )
    (mask_decoder): MLPMaskDecoder(
      (query_mlp): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=240, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (pix_mlp): MLP(
        (layers): ModuleList(
          (0): Conv2d(240, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (attn_mlp): MLP(
        (layers): ModuleList(
          (0): Conv2d(240, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(256, 3072, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (bias_scaling): Linear(in_features=1, out_features=1, bias=True)
    )
  )
  (clip_visual_extractor): FeatureExtractor(
    (patchnorm_pre_ln): Identity()
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (patch_dropout): Identity()
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (3): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (4): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (5): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (6): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (7): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (8): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
    )
  )
  (clip_rec_head): RecWithAttnbiasHead(
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (ov_classifier): LearnableBgOvClassifier(
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (1): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (2): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (3): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (4): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (5): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (6): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (7): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (8): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (9): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (10): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (11): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (token_embedding): Embedding(49408, 512)
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)> to CPU due to CUDA OOM
[12/02 00:58:54 d2.utils.memory]: Attempting to copy inputs of <bound method SAN.semantic_inference of SAN(
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_vq': 1.0, 'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_vq_0': 1.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0}
      num_classes: 171
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (query_proj): Linear(in_features=240, out_features=768, bias=False)
  (side_adapter_network): RegionwiseSideAdapterNetwork(
    (vit_model): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 240, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (patch_drop): Identity()
      (norm_pre): Identity()
      (blocks): Sequential(
        (0): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (1): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (2): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (3): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (4): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (5): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (6): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (7): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
      )
      (fc_norm): Identity()
      (head_drop): Dropout(p=0.0, inplace=False)
      (head): Identity()
      (norm): Identity()
    )
    (fusion_layers): ModuleDict(
      (layer_0): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_1): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_2): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_3): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
    )
    (mask_decoder): MLPMaskDecoder(
      (query_mlp): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=240, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (pix_mlp): MLP(
        (layers): ModuleList(
          (0): Conv2d(240, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (attn_mlp): MLP(
        (layers): ModuleList(
          (0): Conv2d(240, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(256, 3072, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (bias_scaling): Linear(in_features=1, out_features=1, bias=True)
    )
  )
  (clip_visual_extractor): FeatureExtractor(
    (patchnorm_pre_ln): Identity()
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (patch_dropout): Identity()
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (3): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (4): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (5): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (6): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (7): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (8): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
    )
  )
  (clip_rec_head): RecWithAttnbiasHead(
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (ov_classifier): LearnableBgOvClassifier(
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (1): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (2): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (3): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (4): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (5): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (6): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (7): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (8): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (9): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (10): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (11): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (token_embedding): Embedding(49408, 512)
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)> to CPU due to CUDA OOM
[12/02 00:58:55 d2.evaluation.evaluator]: Inference done 396/5000. Dataloading: 0.0018 s/iter. Inference: 0.1225 s/iter. Eval: 0.0141 s/iter. Total: 0.1384 s/iter. ETA=0:10:37
[12/02 00:58:58 d2.utils.memory]: Attempting to copy inputs of <bound method SAN.semantic_inference of SAN(
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_vq': 1.0, 'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_vq_0': 1.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0}
      num_classes: 171
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (query_proj): Linear(in_features=240, out_features=768, bias=False)
  (side_adapter_network): RegionwiseSideAdapterNetwork(
    (vit_model): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 240, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (patch_drop): Identity()
      (norm_pre): Identity()
      (blocks): Sequential(
        (0): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (1): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (2): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (3): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (4): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (5): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (6): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (7): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
      )
      (fc_norm): Identity()
      (head_drop): Dropout(p=0.0, inplace=False)
      (head): Identity()
      (norm): Identity()
    )
    (fusion_layers): ModuleDict(
      (layer_0): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_1): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_2): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_3): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
    )
    (mask_decoder): MLPMaskDecoder(
      (query_mlp): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=240, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (pix_mlp): MLP(
        (layers): ModuleList(
          (0): Conv2d(240, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (attn_mlp): MLP(
        (layers): ModuleList(
          (0): Conv2d(240, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(256, 3072, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (bias_scaling): Linear(in_features=1, out_features=1, bias=True)
    )
  )
  (clip_visual_extractor): FeatureExtractor(
    (patchnorm_pre_ln): Identity()
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (patch_dropout): Identity()
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (3): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (4): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (5): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (6): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (7): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (8): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
    )
  )
  (clip_rec_head): RecWithAttnbiasHead(
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (ov_classifier): LearnableBgOvClassifier(
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (1): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (2): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (3): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (4): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (5): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (6): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (7): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (8): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (9): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (10): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (11): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (token_embedding): Embedding(49408, 512)
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)> to CPU due to CUDA OOM
[12/02 00:59:01 d2.evaluation.evaluator]: Inference done 433/5000. Dataloading: 0.0018 s/iter. Inference: 0.1224 s/iter. Eval: 0.0140 s/iter. Total: 0.1383 s/iter. ETA=0:10:31
[12/02 00:59:01 d2.utils.memory]: Attempting to copy inputs of <bound method SAN.semantic_inference of SAN(
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_vq': 1.0, 'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_vq_0': 1.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0}
      num_classes: 171
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (query_proj): Linear(in_features=240, out_features=768, bias=False)
  (side_adapter_network): RegionwiseSideAdapterNetwork(
    (vit_model): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 240, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (patch_drop): Identity()
      (norm_pre): Identity()
      (blocks): Sequential(
        (0): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (1): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (2): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (3): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (4): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (5): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (6): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (7): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
      )
      (fc_norm): Identity()
      (head_drop): Dropout(p=0.0, inplace=False)
      (head): Identity()
      (norm): Identity()
    )
    (fusion_layers): ModuleDict(
      (layer_0): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_1): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_2): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_3): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
    )
    (mask_decoder): MLPMaskDecoder(
      (query_mlp): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=240, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (pix_mlp): MLP(
        (layers): ModuleList(
          (0): Conv2d(240, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (attn_mlp): MLP(
        (layers): ModuleList(
          (0): Conv2d(240, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(256, 3072, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (bias_scaling): Linear(in_features=1, out_features=1, bias=True)
    )
  )
  (clip_visual_extractor): FeatureExtractor(
    (patchnorm_pre_ln): Identity()
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (patch_dropout): Identity()
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (3): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (4): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (5): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (6): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (7): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (8): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
    )
  )
  (clip_rec_head): RecWithAttnbiasHead(
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (ov_classifier): LearnableBgOvClassifier(
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (1): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (2): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (3): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (4): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (5): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (6): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (7): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (8): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (9): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (10): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (11): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (token_embedding): Embedding(49408, 512)
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)> to CPU due to CUDA OOM
[12/02 00:59:03 d2.utils.memory]: Attempting to copy inputs of <bound method SAN.semantic_inference of SAN(
  (criterion): Criterion SetCriterion
      matcher: Matcher HungarianMatcher
          cost_class: 2.0
          cost_mask: 5.0
          cost_dice: 5.0
      losses: ['labels', 'masks']
      weight_dict: {'loss_vq': 1.0, 'loss_ce': 2.0, 'loss_mask': 5.0, 'loss_dice': 5.0, 'loss_vq_0': 1.0, 'loss_ce_0': 2.0, 'loss_mask_0': 5.0, 'loss_dice_0': 5.0}
      num_classes: 171
      eos_coef: 0.1
      num_points: 12544
      oversample_ratio: 3.0
      importance_sample_ratio: 0.75
  (query_proj): Linear(in_features=240, out_features=768, bias=False)
  (side_adapter_network): RegionwiseSideAdapterNetwork(
    (vit_model): VisionTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 240, kernel_size=(16, 16), stride=(16, 16))
        (norm): Identity()
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (patch_drop): Identity()
      (norm_pre): Identity()
      (blocks): Sequential(
        (0): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (1): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (2): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (3): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (4): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (5): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (6): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
        (7): Block(
          (norm1): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (attn): Attention(
            (qkv): Linear(in_features=240, out_features=720, bias=True)
            (q_norm): Identity()
            (k_norm): Identity()
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=240, out_features=240, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
          )
          (ls1): Identity()
          (drop_path1): Identity()
          (norm2): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=240, out_features=960, bias=True)
            (act): GELU(approximate=none)
            (drop1): Dropout(p=0.0, inplace=False)
            (norm): Identity()
            (fc2): Linear(in_features=960, out_features=240, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (ls2): Identity()
          (drop_path2): Identity()
        )
      )
      (fc_norm): Identity()
      (head_drop): Dropout(p=0.0, inplace=False)
      (head): Identity()
      (norm): Identity()
    )
    (fusion_layers): ModuleDict(
      (layer_0): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_1): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_2): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
      (layer_3): VQFusion(
        (input_proj): Sequential(
          (0): LayerNorm()
          (1): Conv2d(768, 240, kernel_size=(1, 1), stride=(1, 1))
        )
        (cross_attn): MeanShiftTransformer(
          (layers): ModuleList(
            (0): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): ModuleList(
              (0): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (1): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): MeanShiftAttention(
                  (attn_prob): Softmax(dim=-1)
                  (query): Linear(in_features=240, out_features=480, bias=False)
                  (key): Linear(in_features=240, out_features=480, bias=False)
                  (value): Linear(in_features=240, out_features=480, bias=False)
                  (to_out): Sequential(
                    (0): Linear(in_features=480, out_features=240, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                  )
                )
              )
              (2): LayerNormBlock(
                (norm): LayerNorm((240,), eps=1e-06, elementwise_affine=True)
                (dropout): Dropout(p=0.0, inplace=False)
                (fn): FeedForward(
                  (net): Sequential(
                    (0): Linear(in_features=240, out_features=640, bias=True)
                    (1): GELU(approximate=none)
                    (2): Dropout(p=0.0, inplace=False)
                    (3): Linear(in_features=640, out_features=240, bias=True)
                    (4): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
        )
      )
    )
    (mask_decoder): MLPMaskDecoder(
      (query_mlp): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=240, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
          (2): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (pix_mlp): MLP(
        (layers): ModuleList(
          (0): Conv2d(240, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (attn_mlp): MLP(
        (layers): ModuleList(
          (0): Conv2d(240, 256, kernel_size=(1, 1), stride=(1, 1))
          (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
          (2): Conv2d(256, 3072, kernel_size=(1, 1), stride=(1, 1))
        )
      )
      (bias_scaling): Linear(in_features=1, out_features=1, bias=True)
    )
  )
  (clip_visual_extractor): FeatureExtractor(
    (patchnorm_pre_ln): Identity()
    (conv1): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)
    (patch_dropout): Identity()
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (3): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (4): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (5): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (6): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (7): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (8): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
    )
  )
  (clip_rec_head): RecWithAttnbiasHead(
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (ov_classifier): LearnableBgOvClassifier(
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (1): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (2): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (3): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (4): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (5): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (6): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (7): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (8): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (9): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (10): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
        (11): ResidualAttentionBlock(
          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=512, out_features=2048, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=2048, out_features=512, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (token_embedding): Embedding(49408, 512)
    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
)> to CPU due to CUDA OOM
ERROR [12/02 00:59:05 d2.engine.train_loop]: Exception during training:
Traceback (most recent call last):
  File "/home/zjy/.conda/envs/san/lib/python3.9/site-packages/detectron2/engine/train_loop.py", line 156, in train
    self.after_step()
  File "/home/zjy/.conda/envs/san/lib/python3.9/site-packages/detectron2/engine/train_loop.py", line 190, in after_step
    h.after_step()
  File "/home/zjy/.conda/envs/san/lib/python3.9/site-packages/detectron2/engine/hooks.py", line 556, in after_step
    self._do_eval()
  File "/home/zjy/.conda/envs/san/lib/python3.9/site-packages/detectron2/engine/hooks.py", line 529, in _do_eval
    results = self._func()
  File "/home/zjy/.conda/envs/san/lib/python3.9/site-packages/detectron2/engine/defaults.py", line 455, in test_and_save_results
    self._last_eval_results = self.test(self.cfg, self.model)
  File "/home/zjy/.conda/envs/san/lib/python3.9/site-packages/detectron2/engine/defaults.py", line 619, in test
    results_i = inference_on_dataset(model, data_loader, evaluator)
  File "/home/zjy/.conda/envs/san/lib/python3.9/site-packages/detectron2/evaluation/evaluator.py", line 158, in inference_on_dataset
    outputs = model(inputs)
  File "/home/zjy/.conda/envs/san/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/zjy/sources/SAN/san/model/san.py", line 232, in forward
    mask_preds = F.interpolate(
  File "/home/zjy/.conda/envs/san/lib/python3.9/site-packages/torch/nn/functional.py", line 3938, in interpolate
    return torch._C._nn.upsample_bilinear2d(input, output_size, align_corners, scale_factors)
RuntimeError: CUDA out of memory. Tried to allocate 3.20 GiB (GPU 0; 9.78 GiB total capacity; 1.24 GiB already allocated; 2.47 GiB free; 5.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[12/02 00:59:05 d2.engine.hooks]: Overall training speed: 4997 iterations in 0:24:36 (0.2956 s / it)
[12/02 00:59:05 d2.engine.hooks]: Total training time: 0:26:14 (0:01:38 on hooks)
[12/02 00:59:05 d2.utils.events]:  eta: 4:27:51  iter: 4999  total_loss: 17.69  loss_ce: 0.5492  loss_mask: 1.631  loss_dice: 2.757  loss_ce_0: 0.5523  loss_mask_0: 1.671  loss_dice_0: 2.787  loss_vq: 3.688  loss_vq_0: 3.688    time: 0.2955  last_time: 0.2900  data_time: 0.0052  last_data_time: 0.0050   lr: 9.2469e-05  max_mem: 7627M
wandb: Waiting for W&B process to finish... (success).
wandb: üöÄ View run san_wandb_log at: https://wandb.ai/jayzz/oseg_1111/runs/0cfonvyr
wandb: Ô∏è‚ö° View job at https://wandb.ai/jayzz/oseg_1111/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjg1Mjk3NTM2/version_details/v11
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
Traceback (most recent call last):
  File "/data/zjy/sources/SAN/train_net.py", line 285, in <module>
    launch(
  File "/home/zjy/.conda/envs/san/lib/python3.9/site-packages/detectron2/engine/launch.py", line 84, in launch
    main_func(*args)
  File "/data/zjy/sources/SAN/train_net.py", line 279, in main
    return trainer.train()
  File "/home/zjy/.conda/envs/san/lib/python3.9/site-packages/detectron2/engine/defaults.py", line 486, in train
    super().train(self.start_iter, self.max_iter)
  File "/home/zjy/.conda/envs/san/lib/python3.9/site-packages/detectron2/engine/train_loop.py", line 156, in train
    self.after_step()
  File "/home/zjy/.conda/envs/san/lib/python3.9/site-packages/detectron2/engine/train_loop.py", line 190, in after_step
    h.after_step()
  File "/home/zjy/.conda/envs/san/lib/python3.9/site-packages/detectron2/engine/hooks.py", line 556, in after_step
    self._do_eval()
  File "/home/zjy/.conda/envs/san/lib/python3.9/site-packages/detectron2/engine/hooks.py", line 529, in _do_eval
    results = self._func()
  File "/home/zjy/.conda/envs/san/lib/python3.9/site-packages/detectron2/engine/defaults.py", line 455, in test_and_save_results
    self._last_eval_results = self.test(self.cfg, self.model)
  File "/home/zjy/.conda/envs/san/lib/python3.9/site-packages/detectron2/engine/defaults.py", line 619, in test
    results_i = inference_on_dataset(model, data_loader, evaluator)
  File "/home/zjy/.conda/envs/san/lib/python3.9/site-packages/detectron2/evaluation/evaluator.py", line 158, in inference_on_dataset
    outputs = model(inputs)
  File "/home/zjy/.conda/envs/san/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/zjy/sources/SAN/san/model/san.py", line 232, in forward
    mask_preds = F.interpolate(
  File "/home/zjy/.conda/envs/san/lib/python3.9/site-packages/torch/nn/functional.py", line 3938, in interpolate
    return torch._C._nn.upsample_bilinear2d(input, output_size, align_corners, scale_factors)
RuntimeError: CUDA out of memory. Tried to allocate 3.20 GiB (GPU 0; 9.78 GiB total capacity; 1.24 GiB already allocated; 2.47 GiB free; 5.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
